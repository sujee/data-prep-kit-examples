{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG: Milvus + Llama @ Replicate\n",
    "\n",
    "Query markdown documents using LLM.\n",
    "\n",
    "Load markdown documents in   [data/milvus_docs/en/faq](data/milvus_docs/en/faq)\n",
    "\n",
    "References:\n",
    "- https://milvus.io/docs/build-rag-with-milvus.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig:\n",
    "    pass\n",
    "\n",
    "MY_CONFIG = MyConfig()\n",
    "\n",
    "MY_CONFIG.EMBEDDING_MODEL = \"BAAI/bge-small-en-v1.5\"\n",
    "MY_CONFIG.EMBEDDING_LENGTH = 384\n",
    "\n",
    "MY_CONFIG.INPUT_DATA_DIR = 'data/milvus_docs/en/faq'\n",
    "\n",
    "MY_CONFIG.DB_URI = './rag3_milvus_faq.db'\n",
    "MY_CONFIG.COLLECTION_NAME = 'milvus_faq_docs'\n",
    "\n",
    "MY_CONFIG.LLM_MODEL = \"meta/meta-llama-3-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ config REPLICATE_API_TOKEN found\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "## Load Settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# debug\n",
    "# print (config)\n",
    "\n",
    "MY_CONFIG.REPLICATE_API_TOKEN = config.get('REPLICATE_API_TOKEN')\n",
    "\n",
    "if  MY_CONFIG.REPLICATE_API_TOKEN:\n",
    "    print (\"✅ config REPLICATE_API_TOKEN found\")\n",
    "else:\n",
    "    raise Exception (\"'❌ REPLICATE_API_TOKEN' is not set.  Please set it above to continue...\")\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = MY_CONFIG.REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text_lines) 72\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "text_lines = []\n",
    "\n",
    "for file_path in glob(f\"{MY_CONFIG.INPUT_DATA_DIR}/*.md\", recursive=True):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        file_text = file.read()\n",
    "\n",
    "    text_lines += file_text.split(\"# \")\n",
    "\n",
    "print ('len(text_lines)', len(text_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-dev-1/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(MY_CONFIG.EMBEDDING_MODEL)\n",
    "\n",
    "def get_embeddings (str):\n",
    "    embeddings = model.encode(str, normalize_embeddings=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings len = 384\n",
      "embeddings[:5] =  [-0.02412123 -0.02083506  0.03565466  0.00688349  0.02383429]\n"
     ]
    }
   ],
   "source": [
    "# Test embeddings\n",
    "embeddings = get_embeddings('Paris 2024 Olympics')\n",
    "print ('embeddings len =', len(embeddings))\n",
    "print ('embeddings[:5] = ', embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance: ./rag_demo_milvus_faq_1.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(uri=MY_CONFIG.DB_URI)\n",
    "\n",
    "print (\"✅ Connected to Milvus instance:\", MY_CONFIG.DB_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created collection :  milvus_faq_docs\n"
     ]
    }
   ],
   "source": [
    "if milvus_client.has_collection(MY_CONFIG.COLLECTION_NAME):\n",
    "    milvus_client.drop_collection(MY_CONFIG.COLLECTION_NAME)\n",
    "    print ('✅ Cleared collection :', MY_CONFIG.COLLECTION_NAME)\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=MY_CONFIG.COLLECTION_NAME,\n",
    "    dimension=MY_CONFIG.EMBEDDING_LENGTH,\n",
    "    metric_type=\"IP\",  # Inner product distance\n",
    "    consistency_level=\"Strong\",  # Strong consistency level\n",
    ")\n",
    "print (\"✅ Created collection : \", MY_CONFIG.COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting data: 100%|██████████| 72/72 [00:00<00:00, 140.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 72 docs into db\n",
      "Record count in 'milvus_faq_docs' = {'row_count': 72}\n",
      "CPU times: user 522 ms, sys: 4.91 ms, total: 527 ms\n",
      "Wall time: 559 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, line in enumerate(tqdm(text_lines, desc=\"Inserting data\")):\n",
    "    data.append({\"id\": i, \"vector\": get_embeddings(line), \"text\": line})\n",
    "\n",
    "milvus_client.insert(collection_name=MY_CONFIG.COLLECTION_NAME, data=data)\n",
    "\n",
    "print (f'✅ Inserted {len(data)} docs into db')\n",
    "\n",
    "print (f\"Record count in '{MY_CONFIG.COLLECTION_NAME}' =\", milvus_client.get_collection_stats(MY_CONFIG.COLLECTION_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant documents using vector / sementic search\n",
    "\n",
    "def fetch_relevant_documents (query : str) :\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=MY_CONFIG.COLLECTION_NAME,\n",
    "        data = [get_embeddings(query)], # Use the `emb_text` function to convert the question to an embedding vector\n",
    "        limit=3,  # Return top 3 results\n",
    "        search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
    "        output_fields=[\"text\"],  # Return the text field\n",
    "    )\n",
    "    # print (search_res)\n",
    "\n",
    "    retrieved_docs_with_distances = [\n",
    "        {'text': res[\"entity\"][\"text\"], 'distance' : res[\"distance\"]} for res in search_res[0]\n",
    "    ]\n",
    "    return retrieved_docs_with_distances\n",
    "## --- end ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'distance': 0.8521138429641724,\n",
      "        'text': ' Where does Milvus store data?\\n'\n",
      "                '\\n'\n",
      "                'Milvus deals with two types of data, inserted data and '\n",
      "                'metadata. \\n'\n",
      "                '\\n'\n",
      "                'Inserted data, including vector data, scalar data, and '\n",
      "                'collection-specific schema, are stored in persistent storage '\n",
      "                'as incremental log. Milvus supports multiple object storage '\n",
      "                'backends, including [MinIO](https://min.io/), [AWS '\n",
      "                'S3](https://aws.amazon.com/s3/?nc1=h_ls), [Google Cloud '\n",
      "                'Storage](https://cloud.google.com/storage?hl=en#object-storage-for-companies-of-all-sizes) '\n",
      "                '(GCS), [Azure Blob '\n",
      "                'Storage](https://azure.microsoft.com/en-us/products/storage/blobs), '\n",
      "                '[Alibaba Cloud '\n",
      "                'OSS](https://www.alibabacloud.com/product/object-storage-service), '\n",
      "                'and [Tencent Cloud Object '\n",
      "                'Storage](https://www.tencentcloud.com/products/cos) (COS).\\n'\n",
      "                '\\n'\n",
      "                'Metadata are generated within Milvus. Each Milvus module has '\n",
      "                'its own metadata that are stored in etcd.\\n'\n",
      "                '\\n'\n",
      "                '###'},\n",
      "    {   'distance': 0.798244059085846,\n",
      "        'text': 'How does Milvus flush data?\\n'\n",
      "                '\\n'\n",
      "                'Milvus returns success when inserted data are loaded to the '\n",
      "                'message queue. However, the data are not yet flushed to the '\n",
      "                \"disk. Then Milvus' data node writes the data in the message \"\n",
      "                'queue to persistent storage as incremental logs. If `flush()` '\n",
      "                'is called, the data node is forced to write all data in the '\n",
      "                'message queue to persistent storage immediately.\\n'\n",
      "                '\\n'\n",
      "                '###'},\n",
      "    {   'distance': 0.7783681154251099,\n",
      "        'text': 'Does the query perform in memory? What are incremental data '\n",
      "                'and historical data?\\n'\n",
      "                '\\n'\n",
      "                'Yes. When a query request comes, Milvus searches both '\n",
      "                'incremental data and historical data by loading them into '\n",
      "                'memory. Incremental data are in the growing segments, which '\n",
      "                'are buffered in memory before they reach the threshold to be '\n",
      "                'persisted in storage engine, while historical data are from '\n",
      "                'the sealed segments that are stored in the object storage. '\n",
      "                'Incremental data and historical data together constitute the '\n",
      "                'whole dataset to search.\\n'\n",
      "                '\\n'\n",
      "                '###'}]\n"
     ]
    }
   ],
   "source": [
    "# test relevant vector search\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "question = \"How is data stored in milvus?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "pprint.pprint(relevant_docs, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "def ask_LLM (question, relevant_docs):\n",
    "    context = \"\\n\".join(\n",
    "        [doc['text'] for doc in relevant_docs]\n",
    "    )\n",
    "    print ('============ context (this is the context supplied to LLM) ============')\n",
    "    print (context)\n",
    "    print ('============ end  context ============', flush=True)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    \"\"\"\n",
    "\n",
    "    print ('============ here is the answer from LLM... STREAMING... =====')\n",
    "    # The meta/meta-llama-3-8b-instruct model can stream output as it's running.\n",
    "    for event in replicate.stream(\n",
    "        MY_CONFIG.LLM_MODEL,\n",
    "        input={\n",
    "            \"top_k\": 0,\n",
    "            \"top_p\": 0.95,\n",
    "            \"prompt\": user_prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.1,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"length_penalty\": 1,\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"stop_sequences\": \"<|end_of_text|>,<|eot_id|>\",\n",
    "            \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            \"presence_penalty\": 0,\n",
    "            \"log_performance_metrics\": False\n",
    "        },\n",
    "    ):\n",
    "        print(str(event), end=\"\")\n",
    "    ## ---\n",
    "    print ('\\n======  end LLM answer ======\\n', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ context (this is the context supplied to LLM) ============\n",
      " Where does Milvus store data?\n",
      "\n",
      "Milvus deals with two types of data, inserted data and metadata. \n",
      "\n",
      "Inserted data, including vector data, scalar data, and collection-specific schema, are stored in persistent storage as incremental log. Milvus supports multiple object storage backends, including [MinIO](https://min.io/), [AWS S3](https://aws.amazon.com/s3/?nc1=h_ls), [Google Cloud Storage](https://cloud.google.com/storage?hl=en#object-storage-for-companies-of-all-sizes) (GCS), [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), [Alibaba Cloud OSS](https://www.alibabacloud.com/product/object-storage-service), and [Tencent Cloud Object Storage](https://www.tencentcloud.com/products/cos) (COS).\n",
      "\n",
      "Metadata are generated within Milvus. Each Milvus module has its own metadata that are stored in etcd.\n",
      "\n",
      "###\n",
      "How does Milvus flush data?\n",
      "\n",
      "Milvus returns success when inserted data are loaded to the message queue. However, the data are not yet flushed to the disk. Then Milvus' data node writes the data in the message queue to persistent storage as incremental logs. If `flush()` is called, the data node is forced to write all data in the message queue to persistent storage immediately.\n",
      "\n",
      "###\n",
      "Does the query perform in memory? What are incremental data and historical data?\n",
      "\n",
      "Yes. When a query request comes, Milvus searches both incremental data and historical data by loading them into memory. Incremental data are in the growing segments, which are buffered in memory before they reach the threshold to be persisted in storage engine, while historical data are from the sealed segments that are stored in the object storage. Incremental data and historical data together constitute the whole dataset to search.\n",
      "\n",
      "###\n",
      "============ end  context ============\n",
      "============ here is the answer from LLM... STREAMING... =====\n",
      "According to the provided context, Milvus stores data in the following ways:\n",
      "\n",
      "* Inserted data, including vector data, scalar data, and collection-specific schema, are stored in persistent storage as incremental logs.\n",
      "* Metadata are generated within Milvus and are stored in etcd.\n",
      "\n",
      "Additionally, incremental data and historical data are loaded into memory when a query request comes, and the query searches both types of data.\n",
      "======  end LLM answer ======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"How is data stored in milvus?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-prep-kit-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
