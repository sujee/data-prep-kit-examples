{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with IlamaIndex + Milvus + Llama @ Replicate\n",
    "\n",
    "References\n",
    "- https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo/\n",
    "- https://docs.llamaindex.ai/en/stable/api_reference/storage/vector_store/milvus/?h=milvusvectorstore#llama_index.vector_stores.milvus.MilvusVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig:\n",
    "    pass\n",
    "\n",
    "MY_CONFIG = MyConfig()\n",
    "\n",
    "MY_CONFIG.EMBEDDING_MODEL = \"BAAI/bge-small-en-v1.5\"\n",
    "MY_CONFIG.EMBEDDING_LENGTH = 384\n",
    "\n",
    "MY_CONFIG.INPUT_DATA_DIR = 'data/granite-docs/input'\n",
    "\n",
    "MY_CONFIG.DB_URI = './rag2_granitedocs_llamaindex.db'\n",
    "MY_CONFIG.COLLECTION_NAME = 'llamaindex_granite_docs'\n",
    "\n",
    "MY_CONFIG.LLM_MODEL = \"meta/meta-llama-3-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 chunks\n",
      "Document [0].doc_id: 68d3ea02-10b4-4229-8e51-9f57a2607c2f\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "import pprint\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir = MY_CONFIG.INPUT_DATA_DIR\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(documents)} chunks\")\n",
    "\n",
    "print(\"Document [0].doc_id:\", documents[0].doc_id)\n",
    "# pprint.pprint (documents[0], indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-dev-1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name = MY_CONFIG.EMBEDDING_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance: ./rag2_granitedocs_llamaindex.db\n",
      "✅ Cleared collection : llamaindex_granite_docs\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(MY_CONFIG.DB_URI)\n",
    "print (\"✅ Connected to Milvus instance:\", MY_CONFIG.DB_URI)\n",
    "\n",
    "\n",
    "# if we already have a collection, clear it first\n",
    "if milvus_client.has_collection(collection_name = MY_CONFIG.COLLECTION_NAME):\n",
    "    milvus_client.drop_collection(collection_name = MY_CONFIG.COLLECTION_NAME)\n",
    "    print ('✅ Cleared collection :', MY_CONFIG.COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created collection : llamaindex_granite_docs\n"
     ]
    }
   ],
   "source": [
    "# connect to vector db\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri = MY_CONFIG.DB_URI ,\n",
    "    dim = MY_CONFIG.EMBEDDING_LENGTH , \n",
    "    collection_name = MY_CONFIG.COLLECTION_NAME,\n",
    "    overwrite=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "print (\"✅ Created collection :\", MY_CONFIG.COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 814 ms, sys: 125 ms, total: 939 ms\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create an index\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 341 ms, sys: 11.4 ms, total: 352 ms\n",
      "Wall time: 817 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create an index\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llamaindex_granite_docs']\n",
      "---------\n",
      "{'aliases': [],\n",
      " 'auto_id': False,\n",
      " 'collection_id': 0,\n",
      " 'collection_name': 'llamaindex_granite_docs',\n",
      " 'consistency_level': 0,\n",
      " 'description': '',\n",
      " 'enable_dynamic_field': True,\n",
      " 'fields': [{'description': '',\n",
      "             'field_id': 100,\n",
      "             'is_primary': True,\n",
      "             'name': 'id',\n",
      "             'params': {'max_length': 65535},\n",
      "             'type': <DataType.VARCHAR: 21>},\n",
      "            {'description': '',\n",
      "             'field_id': 101,\n",
      "             'name': 'embedding',\n",
      "             'params': {'dim': 384},\n",
      "             'type': <DataType.FLOAT_VECTOR: 101>}],\n",
      " 'num_partitions': 0,\n",
      " 'num_shards': 0,\n",
      " 'properties': {}}\n"
     ]
    }
   ],
   "source": [
    "# See data in vector db\n",
    "\n",
    "from pymilvus import MilvusClient\n",
    "import pprint \n",
    "\n",
    "milvus_client = MilvusClient(MY_CONFIG.DB_URI)\n",
    "res = milvus_client.list_collections()\n",
    "\n",
    "print(res)\n",
    "print ('---------')\n",
    "\n",
    "res = milvus_client.describe_collection(\n",
    "    collection_name = MY_CONFIG.COLLECTION_NAME\n",
    ")\n",
    "\n",
    "pprint.pprint(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## Load Settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = config.get('REPLICATE_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.replicate import Replicate\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = Replicate(\n",
    "    model= MY_CONFIG.LLM_MODEL,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Based on the provided context information, I'll summarize the document for you in one paragraph. The document appears to be a release notes or change log for a project called \"Granite\". The document outlines the updates and changes made to the project from September 15th, 2023, to March 12th, 2024. The updates include changes to tables, grammar corrections, and the addition of new documentation on the granite.13b.v2 models. The document also mentions the inclusion of new pre-training datasets, model safety and red-teaming benchmarks, and evaluation results for the granite.13b.v2 model. Additionally, the document includes updates on the latest granite model training approach and results, as well as corrections to the AttaQ table and figures.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"Summarize this document for me in one paragraph\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Based on the provided context information, the training dataset for the granite.13b model is a combination of 14 datasets, which are:\n",
      "\n",
      "1. arXiv\n",
      "2. Common Crawl\n",
      "3. DeepMind Mathematics\n",
      "4. Free Law\n",
      "5. GitHub Clean\n",
      "6. Hacker News\n",
      "7. OpenWeb Text\n",
      "8. Project Gutenberg (PG-19)\n",
      "9. Pubmed Central\n",
      "10. SEC Filings\n",
      "11. Stack Exchange\n",
      "12. USPTO\n",
      "13. Webhose\n",
      "14. Wikimedia\n",
      "\n",
      "Additionally, the second version of the base model, granite.13b.v2, continued pre-training on an additional 1.5T newly-curated tokens, which included a mixture of the same 14 datasets from granite.13b.v1, along with 6 new datasets.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"What was the training dataset?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm happy to help! However, I don't see any information about the moon landing in the provided context. The text appears to be about AI models, training, and infrastructure. Therefore, I cannot provide an answer to the query about the moon landing. If you have any other questions or queries related to the provided context, I'll do my best to assist you!\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"When was the moon landing?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-prep-kit-dev-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
