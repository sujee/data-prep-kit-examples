{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Data using LLM\n",
    "\n",
    "Here is the overall RAG pipeline.   In this notebook, we will do steps (5), (6), (7), (8), (9)\n",
    "- Importing data is already done in this notebook [rag_1_B_load_data.ipynb](rag_1_B_load_data.ipynb)\n",
    "- üëâ Step 5: Calculate embedding for user query\n",
    "- üëâ Step 6 & 7: Send the query to vector db to retrieve relevant documents\n",
    "- üëâ Step 8 & 9: Send the query and relevant documents (returned above step) to LLM and get answers to our query\n",
    "\n",
    "![image missing](../media/rag-overview-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig:\n",
    "    pass\n",
    "MY_CONFIG = MyConfig()\n",
    "\n",
    "MY_CONFIG.EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MY_CONFIG.EMBEDDING_LENGTH = 384\n",
    "\n",
    "MY_CONFIG.DB_URI = './rag_4_walmart_dataprepkit.db'  # For embedded instance\n",
    "#MY_CONFIG.DB_URI = 'http://localhost:19530'  # For Docker instance\n",
    "MY_CONFIG.COLLECTION_NAME = 'dataprepkit_walmart_docs'\n",
    "\n",
    "MY_CONFIG.LLM_MODEL = \"meta/meta-llama-3-8b-instruct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Create a .env file with the following properties.  You can use [env.txt](../env.txt) as starting point\n",
    "\n",
    "---\n",
    "\n",
    "```text\n",
    "REPLICATE_API_TOKEN=YOUR_TOKEN_GOES_HERE\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ config REPLICATE_API_TOKEN found\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "## Load Settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# debug\n",
    "# print (config)\n",
    "\n",
    "MY_CONFIG.REPLICATE_API_TOKEN = config.get('REPLICATE_API_TOKEN')\n",
    "\n",
    "if  MY_CONFIG.REPLICATE_API_TOKEN:\n",
    "    print (\"‚úÖ config REPLICATE_API_TOKEN found\")\n",
    "else:\n",
    "    raise Exception (\"'‚ùå REPLICATE_API_TOKEN' is not set.  Please set it above to continue...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Vector Database\n",
    "\n",
    "Milvus can be embedded and easy to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Milvus instance: ./rag_4_walmart_dataprepkit.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(MY_CONFIG.DB_URI)\n",
    "\n",
    "print (\"‚úÖ Connected to Milvus instance:\", MY_CONFIG.DB_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-: Setup Embeddings\n",
    "\n",
    "Use the same embeddings we used to index our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-dev-1/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(MY_CONFIG.EMBEDDING_MODEL)\n",
    "\n",
    "def get_embeddings (str):\n",
    "    embeddings = model.encode(str, normalize_embeddings=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings len = 384\n",
      "embeddings[:5] =  [ 0.02468892  0.10352131  0.0275264  -0.08551715 -0.01412829]\n"
     ]
    }
   ],
   "source": [
    "# Test embeddings\n",
    "embeddings = get_embeddings('Paris 2024 Olympics')\n",
    "print ('embeddings len =', len(embeddings))\n",
    "print ('embeddings[:5] = ', embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant documents using vector / sementic search\n",
    "\n",
    "def fetch_relevant_documents (query : str) :\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=MY_CONFIG.COLLECTION_NAME,\n",
    "        data = [get_embeddings(query)], # Use the `emb_text` function to convert the question to an embedding vector\n",
    "        limit=3,  # Return top 3 results\n",
    "        search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
    "        output_fields=[\"text\"],  # Return the text field\n",
    "    )\n",
    "    # print (search_res)\n",
    "\n",
    "    retrieved_docs_with_distances = [\n",
    "        {'text': res[\"entity\"][\"text\"], 'distance' : res[\"distance\"]} for res in search_res[0]\n",
    "    ]\n",
    "    return retrieved_docs_with_distances\n",
    "## --- end ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'distance': 0.7401622533798218,\n",
      "        'text': 'Strong, Efficient Growth\\n'\n",
      "                'Comparable sales in the U.S., including fuel, increased 8.2% '\n",
      "                'and 7.7% in fiscal 2023 and 2022, respectively, when compared '\n",
      "                'to the previous fiscal year. Walmart U.S. comparable sales '\n",
      "                'increased 7.0% and 6.4% in fiscal 2023 and 2022, '\n",
      "                'respectively. For'},\n",
      "    {   'distance': 0.7293714284896851,\n",
      "        'text': 'General\\n'\n",
      "                'Our operations comprise three reportable segments: Walmart '\n",
      "                \"U.S., Walmart International and Sam's Club. Our fiscal year \"\n",
      "                'ends on January 31 for our United States (\"U.S.\") and '\n",
      "                'Canadian operations. We consolidate all other operations '\n",
      "                'generally using a one-month lag and on a calendar year basis. '\n",
      "                'Our discussion is as of and for the fiscal years ended '\n",
      "                'January 31, 2023 (\"fiscal 2023\"), January 31, 2022 (\"fiscal '\n",
      "                '2022\") and January 31, 2021 (\"fiscal 2021\"). During fiscal '\n",
      "                '2023, we generated total revenues of $611.3 billion, which '\n",
      "                'was comprised primarily of net sales of $605.9 billion.'},\n",
      "    {   'distance': 0.7258474230766296,\n",
      "        'text': '(A(Amounts in millions)\\n'\n",
      "                \"Of Walmart U.S.'s total net sales, approximately $53.4 \"\n",
      "                'billion, $47.8 billion and $43.0 billion related to eCommerce '\n",
      "                'for fiscal 2023, 2022 and 2021, respectively.'}]\n"
     ]
    }
   ],
   "source": [
    "# test relevant vector search\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "question = \"What was Walmart's revenue in 2023?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "pprint.pprint(relevant_docs, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM\n",
    "\n",
    "### LLM Choices at Replicate\n",
    "\n",
    "- llama 3.1 : Latest\n",
    "    - **meta/meta-llama-3.1-405b-instruct** : Meta's flagship 405 billion parameter language model, fine-tuned for chat completions\n",
    "- Base version of llama-3 from meta\n",
    "    - [meta/meta-llama-3-8b](https://replicate.com/meta/meta-llama-3-8b) : Base version of Llama 3, an 8 billion parameter language model from Meta.\n",
    "    - **meta/meta-llama-3-70b** : 70 billion\n",
    "- Instruct versions of llama-3 from meta, fine tuned for chat completions\n",
    "    - **meta/meta-llama-3-8b-instruct** : An 8 billion parameter language model from Meta, \n",
    "    - **meta/meta-llama-3-70b-instruct** : 70 billion\n",
    "\n",
    "References \n",
    "\n",
    "- https://docs.llamaindex.ai/en/stable/examples/llm/llama_2/?h=replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = MY_CONFIG.REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "def ask_LLM (question, relevant_docs):\n",
    "    context = \"\\n\".join(\n",
    "        [doc['text'] for doc in relevant_docs]\n",
    "    )\n",
    "    print ('============ context (this is the context supplied to LLM) ============')\n",
    "    print (context)\n",
    "    print ('============ end  context ============', flush=True)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    \"\"\"\n",
    "\n",
    "    print ('============ here is the answer from LLM... STREAMING... =====')\n",
    "    # The meta/meta-llama-3-8b-instruct model can stream output as it's running.\n",
    "    for event in replicate.stream(\n",
    "        MY_CONFIG.LLM_MODEL,\n",
    "        input={\n",
    "            \"top_k\": 0,\n",
    "            \"top_p\": 0.95,\n",
    "            \"prompt\": user_prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.1,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"length_penalty\": 1,\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"stop_sequences\": \"<|end_of_text|>,<|eot_id|>\",\n",
    "            \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            \"presence_penalty\": 0,\n",
    "            \"log_performance_metrics\": False\n",
    "        },\n",
    "    ):\n",
    "        print(str(event), end=\"\")\n",
    "    ## ---\n",
    "    print ('\\n======  end LLM answer ======\\n', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "def ask_LLM (question, relevant_docs):\n",
    "    context = \"\\n\".join(\n",
    "        [doc['text'] for doc in relevant_docs]\n",
    "    )\n",
    "    print ('============ context (this is the context supplied to LLM) ============')\n",
    "    print (context)\n",
    "    print ('============ end  context ============', flush=True)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    \"\"\"\n",
    "\n",
    "    print ('============ here is the answer from LLM... STREAMING... =====')\n",
    "    # The meta/meta-llama-3-8b-instruct model can stream output as it's running.\n",
    "    for event in replicate.stream(\n",
    "        MY_CONFIG.LLM_MODEL,\n",
    "        input={\n",
    "            \"top_k\": 0,\n",
    "            \"top_p\": 0.95,\n",
    "            \"prompt\": user_prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.1,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"length_penalty\": 1,\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"stop_sequences\": \"<|end_of_text|>,<|eot_id|>\",\n",
    "            \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            \"presence_penalty\": 0,\n",
    "            \"log_performance_metrics\": False\n",
    "        },\n",
    "    ):\n",
    "        print(str(event), end=\"\")\n",
    "    ## ---\n",
    "    print ('\\n======  end LLM answer ======\\n', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ context (this is the context supplied to LLM) ============\n",
      "Strong, Efficient Growth\n",
      "Comparable sales in the U.S., including fuel, increased 8.2% and 7.7% in fiscal 2023 and 2022, respectively, when compared to the previous fiscal year. Walmart U.S. comparable sales increased 7.0% and 6.4% in fiscal 2023 and 2022, respectively. For\n",
      "General\n",
      "Our operations comprise three reportable segments: Walmart U.S., Walmart International and Sam's Club. Our fiscal year ends on January 31 for our United States (\"U.S.\") and Canadian operations. We consolidate all other operations generally using a one-month lag and on a calendar year basis. Our discussion is as of and for the fiscal years ended January 31, 2023 (\"fiscal 2023\"), January 31, 2022 (\"fiscal 2022\") and January 31, 2021 (\"fiscal 2021\"). During fiscal 2023, we generated total revenues of $611.3 billion, which was comprised primarily of net sales of $605.9 billion.\n",
      "(A(Amounts in millions)\n",
      "Of Walmart U.S.'s total net sales, approximately $53.4 billion, $47.8 billion and $43.0 billion related to eCommerce for fiscal 2023, 2022 and 2021, respectively.\n",
      "============ end  context ============\n",
      "============ here is the answer from LLM... STREAMING... =====\n",
      "According to the provided context, Walmart's total revenue in 2023 was $611.3 billion.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 46.1 ms, sys: 8.61 ms, total: 54.7 ms\n",
      "Wall time: 634 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"What was Walmart's revenue in 2023?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ context (this is the context supplied to LLM) ============\n",
      "Walmart U.S. Segment\n",
      "Distribution . We continue to invest in supply chain automation and utilize a total of 163 distribution facilities which are located strategically throughout the U.S. For fiscal 2023, the majority of Walmart U.S.'s purchases of store merchandise were shipped through these facilities, while most of the remaining store merchandise we purchased was shipped directly from suppliers. General merchandise and dry grocery merchandise is transported primarily through the segment's private truck fleet; however, we contract with common carriers to transport the majority of our perishable grocery merchandise. We ship merchandise purchased by customers on our eCommerce platforms by a number of methods from multiple locations including from our 34 dedicated eCommerce fulfillment centers, as well as leveraging our ability to ship or deliver directly from more than 3,900 stores.\n",
      "Walmart International Segment\n",
      "Distribution. We utilize a total of 188 distribution facilities located in Canada, Central America, Chile, China, India, Mexico and South Africa. Through these facilities, we process and distribute both imported and domestic products to the operating units of the Walmart International segment. During fiscal 2023, the majority of Walmart International's purchases passed through these distribution facilities. Suppliers ship the remainder of Walmart International's purchases directly to our stores in the various markets in which we operate. Across the segment, we have efficient networks connecting physical stores and distribution and fulfillment centers which facilitate the movement of goods to where our customers live. We ship merchandise purchased by customers on our eCommerce platforms by a number of methods from multiple locations including from our 100 dedicated eCommerce fulfillment centers, more than 3,600 eCommerce sort centers and last-mile delivery facilities in India, as well as our physical retail stores.\n",
      "I, C. Douglas McMillon, certify that:\n",
      "1. I have reviewed this Annual Report on Form 10-K of Walmart Inc. (the \"registrant\");\n",
      "============ end  context ============\n",
      "============ here is the answer from LLM... STREAMING... =====\n",
      "According to the provided context, Walmart has a total of:\n",
      "\n",
      "* 163 distribution facilities in the Walmart U.S. Segment\n",
      "* 188 distribution facilities in the Walmart International Segment\n",
      "\n",
      "So, in total, Walmart has 163 + 188 = 351 distribution centers.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 58.6 ms, sys: 16.8 ms, total: 75.5 ms\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"How many distribution centers does Walmart have?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ context (this is the context supplied to LLM) ============\n",
      "Contingencies\n",
      "We have served as the Company's auditor since 1969.\n",
      "                                                 -                                                                        \n",
      "3/29/2024 10:28:40 AM\n",
      "SIGNATURES\n",
      "Pursuant to the requirements of the Securities Exchange Act of 1934, this report has been signed below by the following persons on behalf of the registrant and in the capacities and on the dates indicated:\n",
      "============ end  context ============\n",
      "============ here is the answer from LLM... STREAMING... =====\n",
      "I'm happy to help! However, I don't see any information about the moon landing in the provided context. The context appears to be related to a company's auditor and a report signed by certain individuals. Therefore, I cannot provide an answer to the question about the moon landing. If you could provide more context or clarify the question, I'd be happy to try and assist you further!\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 50.6 ms, sys: 17.8 ms, total: 68.4 ms\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"When was the moon landing?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-prep-kit-dev-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
