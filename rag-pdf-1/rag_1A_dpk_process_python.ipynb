{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Data Processing for RAG with Data Prep Kit (Python)</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15976e3",
   "metadata": {},
   "source": [
    "## Before Running the notebook\n",
    "\n",
    "Please complete [setting up python dev environment](./setup-python-dev-env.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will process PDF documents as part of RAG pipeline\n",
    "\n",
    "![](media/rag-overview-2.png)\n",
    "\n",
    "This notebook will perform steps 1, 2 and 3 in RAG pipeline.\n",
    "\n",
    "Here are the processing steps:\n",
    "\n",
    "- **pdf2parquet** : Extract text from PDF and convert them into parquet files\n",
    "- **Chunk documents**: Split the PDFs into 'meaningful sections' (paragraphs, sentences ..etc)\n",
    "- **Doc_ID generation**: Each chunk is assigned a uniq id, based on content and hash\n",
    "- **Exact Dedup**: Chunks with exact same content are filtered out\n",
    "- **Fuzzy Dedup**: Eliminate chunks that are 'very similar' content\n",
    "- **Doc quality**: Scores the documents based on criteria like number of words, if it contains bad words ..etc\n",
    "- **Text encoder**: Convert chunks into vectors using embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b10be1",
   "metadata": {},
   "source": [
    "## Step-1: Configuration\n",
    "\n",
    "### 1.1 - Common Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33345487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc3f0e",
   "metadata": {},
   "source": [
    "### 1.2 - Inspect Input Data\n",
    "\n",
    "We have a bnunch of datasets in [data](../data) folder.  Examine them\n",
    "\n",
    "We will use one of them or feel free to bring your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### 1.3 - Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleared output directory\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(MY_CONFIG.INPUT_DATA_DIR ):\n",
    "    raise Exception (f\"‚ùå Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
    "\n",
    "output_parquet_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '01_parquet_out')\n",
    "output_chunk_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '02_chunk_out')\n",
    "output_docid_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '03_docid_out')\n",
    "output_exact_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '04_exact_dedupe_out')\n",
    "output_fuzzy_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '05_fuzzy_dedupe_out')\n",
    "output_embeddings_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '06_embeddings_out')\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print (\"‚úÖ Cleared output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d976e-cb4c-4469-af39-4b7ea507e9d8",
   "metadata": {},
   "source": [
    "### 1.4 -  Import Common python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66178913-42b8-426b-a2e9-9587268fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
    "from data_processing.utils import ParamsUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "<a id=\"pdf2parquet\"></a>\n",
    "\n",
    "## Step-2: pdf2parquet -  Convert data from PDF to Parquet\n",
    "\n",
    "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
    "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### 2.1 -  Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-1: Processing input='../data/papers' --> output='output-papers/01_parquet_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE = 1 \n",
    "\n",
    "input_folder = MY_CONFIG.INPUT_DATA_DIR\n",
    "output_folder =  output_parquet_dir\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### 2.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:33:21 INFO - pdf2parquet parameters are : {'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.JSON: 'application/json'>, 'do_table_structure': True, 'do_ocr': True, 'double_precision': 8}\n",
      "14:33:21 INFO - pipeline id pipeline_id\n",
      "14:33:21 INFO - code location None\n",
      "14:33:21 INFO - data factory data_ is using local data access: input_folder - ../data/papers output_folder - output-papers/01_parquet_out\n",
      "14:33:21 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:33:21 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "14:33:21 INFO - orchestrator pdf2parquet started at 2024-09-20 14:33:21\n",
      "14:33:21 INFO - Number of files is 2, source profile {'max_file_size': 5.398899078369141, 'min_file_size': 2.112621307373047, 'total_file_size': 7.5115203857421875}\n",
      "14:33:21 INFO - Initializing models\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a669ceddae742b299dba77cc9b64e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:33:55 INFO - Completed 1 files (50.0%) in 0.499 min\n",
      "14:34:12 INFO - Completed 2 files (100.0%) in 0.79 min\n",
      "14:34:12 INFO - Done processing 2 files, waiting for flush() completion.\n",
      "14:34:12 INFO - done flushing in 0.0 sec\n",
      "14:34:12 INFO - Completed execution in 0.854 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:1 completed successfully\n",
      "CPU times: user 1min 51s, sys: 1.79 s, total: 1min 52s\n",
      "Wall time: 54.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pdf2parquet_transform import (\n",
    "    pdf2parquet_contents_type_cli_param,\n",
    "    pdf2parquet_contents_types,\n",
    ")\n",
    "from pdf2parquet_transform_python import Pdf2ParquetPythonTransformConfiguration\n",
    "\n",
    "from data_processing.utils import GB, ParamsUtils\n",
    "\n",
    "\n",
    "# create parameters\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "ingest_config = {\n",
    "    pdf2parquet_contents_type_cli_param: pdf2parquet_contents_types.JSON,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    \"data_files_to_use\": ast.literal_eval(\"['.pdf']\"),\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
    "# create launcher\n",
    "launcher = PythonTransformLauncher(Pdf2ParquetPythonTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Job failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca790e0",
   "metadata": {},
   "source": [
    "### 2.3 -  Inspect Generated output\n",
    "\n",
    "Here we should see one entry per input file processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe59563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensions (rows x columns)=  (2, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>802bded3-330b-48d7-bb52-1098500decec</td>\n",
       "      <td>pdf</td>\n",
       "      <td>414d95bff49753a945e0917d47b840bef75c67bd56af13...</td>\n",
       "      <td>137574</td>\n",
       "      <td>2024-09-20T14:34:12.950537</td>\n",
       "      <td>17.431519</td>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filename  \\\n",
       "0  attention-is-all-you-need.pdf   \n",
       "1  Granite_Foundation_Models.pdf   \n",
       "\n",
       "                                            contents  num_pages  num_tables  \\\n",
       "0  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...         15           4   \n",
       "1  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...         20          13   \n",
       "\n",
       "   num_doc_elements                           document_id  ext  \\\n",
       "0               193  802bded3-330b-48d7-bb52-1098500decec  pdf   \n",
       "1               444  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "\n",
       "                                                hash    size  \\\n",
       "0  414d95bff49753a945e0917d47b840bef75c67bd56af13...  137574   \n",
       "1  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "\n",
       "                date_acquired  pdf_convert_time                source_filename  \n",
       "0  2024-09-20T14:34:12.950537         17.431519  attention-is-all-you-need.pdf  \n",
       "1  2024-09-20T14:33:55.489893         29.919255  Granite_Foundation_Models.pdf  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Output dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.head(5)\n",
    "\n",
    "## To display certain columns\n",
    "#parquet_df[['column1', 'column2', 'column3']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72274586",
   "metadata": {},
   "source": [
    "<a id=\"chunking\"></a>\n",
    "\n",
    "##  Step-3: Doc chunks\n",
    "\n",
    "Split the documents in chunks, according to their layout segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96198fa6",
   "metadata": {},
   "source": [
    "### 3.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305f00a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-2: Processing input='output-papers/01_parquet_out' --> output='output-papers/02_chunk_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE = 2\n",
    "\n",
    "input_folder = output_parquet_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_chunk_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f2cd1",
   "metadata": {},
   "source": [
    "### 3.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7b18d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34:14 INFO - doc_chunk parameters are : {'chunking_type': <chunking_types.DL_JSON: 'dl_json'>, 'content_column_name': 'contents', 'output_chunk_column_name': 'contents', 'output_jsonpath_column_name': 'doc_jsonpath', 'output_pageno_column_name': 'page_number', 'output_bbox_column_name': 'bbox'}\n",
      "14:34:14 INFO - pipeline id pipeline_id\n",
      "14:34:14 INFO - code location None\n",
      "14:34:14 INFO - data factory data_ is using local data access: input_folder - output-papers/01_parquet_out output_folder - output-papers/02_chunk_out\n",
      "14:34:14 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:34:14 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "14:34:14 INFO - orchestrator doc_chunk started at 2024-09-20 14:34:14\n",
      "14:34:14 INFO - Number of files is 2, source profile {'max_file_size': 0.0879373550415039, 'min_file_size': 0.03559398651123047, 'total_file_size': 0.12353134155273438}\n",
      "14:34:14 INFO - Completed 1 files (50.0%) in 0.001 min\n",
      "14:34:14 INFO - Completed 2 files (100.0%) in 0.001 min\n",
      "14:34:14 INFO - Done processing 2 files, waiting for flush() completion.\n",
      "14:34:14 INFO - done flushing in 0.0 sec\n",
      "14:34:14 INFO - Completed execution in 0.001 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:2 completed successfully\n",
      "CPU times: user 1.04 s, sys: 169 ms, total: 1.21 s\n",
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Import doc_json_chunk transform configuration\n",
    "from doc_chunk_transform_python import DocChunkPythonTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # doc_chunk arguments\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = PythonTransformLauncher(DocChunkPythonTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213afdf6",
   "metadata": {},
   "source": [
    "### 3.3 - Inspect Generated output\n",
    "\n",
    "We would see documents are split into many chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8138d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed : 2\n",
      "Chunks created : 266\n",
      "Input data dimensions (rows x columns)=  (2, 12)\n",
      "Output data dimensions (rows x columns)=  (266, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>802bded3-330b-48d7-bb52-1098500decec</td>\n",
       "      <td>pdf</td>\n",
       "      <td>414d95bff49753a945e0917d47b840bef75c67bd56af13...</td>\n",
       "      <td>137574</td>\n",
       "      <td>2024-09-20T14:34:12.950537</td>\n",
       "      <td>17.431519</td>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>3 Model Architecture\\nFigure 1: The Transforme...</td>\n",
       "      <td>$.main-text[30]</td>\n",
       "      <td>3</td>\n",
       "      <td>[209.53666687, 377.54708862, 401.99035645, 387...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>VI. SOCIO-TECHNICAL HARMS AND RISKS\\nNumerous ...</td>\n",
       "      <td>$.main-text[219]</td>\n",
       "      <td>11</td>\n",
       "      <td>[47.95080566, 281.84732056, 300.52746582, 363....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>A. Overview of Capabilities\\nThe 13b in the na...</td>\n",
       "      <td>$.main-text[9]</td>\n",
       "      <td>1</td>\n",
       "      <td>[48.07516098, 43.18538284, 300.03152466, 65.59...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  num_pages  num_tables  num_doc_elements  \\\n",
       "18   attention-is-all-you-need.pdf         15           4               193   \n",
       "201  Granite_Foundation_Models.pdf         20          13               444   \n",
       "93   Granite_Foundation_Models.pdf         20          13               444   \n",
       "\n",
       "                              document_id  ext  \\\n",
       "18   802bded3-330b-48d7-bb52-1098500decec  pdf   \n",
       "201  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "93   7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "18   414d95bff49753a945e0917d47b840bef75c67bd56af13...  137574   \n",
       "201  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "93   3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "\n",
       "                  date_acquired  pdf_convert_time  \\\n",
       "18   2024-09-20T14:34:12.950537         17.431519   \n",
       "201  2024-09-20T14:33:55.489893         29.919255   \n",
       "93   2024-09-20T14:33:55.489893         29.919255   \n",
       "\n",
       "                   source_filename  \\\n",
       "18   attention-is-all-you-need.pdf   \n",
       "201  Granite_Foundation_Models.pdf   \n",
       "93   Granite_Foundation_Models.pdf   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "18   3 Model Architecture\\nFigure 1: The Transforme...   $.main-text[30]   \n",
       "201  VI. SOCIO-TECHNICAL HARMS AND RISKS\\nNumerous ...  $.main-text[219]   \n",
       "93   A. Overview of Capabilities\\nThe 13b in the na...    $.main-text[9]   \n",
       "\n",
       "     page_number                                               bbox  \n",
       "18             3  [209.53666687, 377.54708862, 401.99035645, 387...  \n",
       "201           11  [47.95080566, 281.84732056, 300.52746582, 363....  \n",
       "93             1  [48.07516098, 43.18538284, 300.03152466, 65.59...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (f\"Files processed : {input_df.shape[0]:,}\")\n",
    "print (f\"Chunks created : {output_df.shape[0]:,}\")\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece021fd",
   "metadata": {},
   "source": [
    "## Step-4:  DOC ID generation\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is a pre-requisite for fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e414c12c",
   "metadata": {},
   "source": [
    "### 4.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10251d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-3: Processing input='output-papers/02_chunk_out' --> output='output-papers/03_docid_out'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "STAGE  = 3\n",
    "\n",
    "input_folder = output_chunk_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_docid_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f312347",
   "metadata": {},
   "source": [
    "### 4.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8b76a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34:14 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'chunk_hash', 'int_column': 'chunk_id', 'start_id': 0}\n",
      "14:34:14 INFO - pipeline id pipeline_id\n",
      "14:34:14 INFO - code location None\n",
      "14:34:14 INFO - data factory data_ is using local data access: input_folder - output-papers/02_chunk_out output_folder - output-papers/03_docid_out\n",
      "14:34:14 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:34:14 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "14:34:14 INFO - orchestrator doc_id started at 2024-09-20 14:34:14\n",
      "14:34:14 INFO - Number of files is 2, source profile {'max_file_size': 0.04591846466064453, 'min_file_size': 0.024618148803710938, 'total_file_size': 0.07053661346435547}\n",
      "14:34:14 INFO - Completed 1 files (50.0%) in 0.0 min\n",
      "14:34:14 INFO - Completed 2 files (100.0%) in 0.0 min\n",
      "14:34:14 INFO - Done processing 2 files, waiting for flush() completion.\n",
      "14:34:14 INFO - done flushing in 0.0 sec\n",
      "14:34:14 INFO - Completed execution in 0.0 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:3 completed successfully\n",
      "CPU times: user 18.9 ms, sys: 4.39 ms, total: 23.3 ms\n",
      "Wall time: 19.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from doc_id_transform_python import DocIDPythonTransformRuntimeConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"chunk_hash\",\n",
    "    \"doc_id_int_column\": \"chunk_id\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = PythonTransformLauncher(DocIDPythonTransformRuntimeConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23338b",
   "metadata": {},
   "source": [
    "### 4.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec23aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (266, 15)\n",
      "Output data dimensions (rows x columns)=  (266, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>chunk_hash</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>802bded3-330b-48d7-bb52-1098500decec</td>\n",
       "      <td>pdf</td>\n",
       "      <td>414d95bff49753a945e0917d47b840bef75c67bd56af13...</td>\n",
       "      <td>137574</td>\n",
       "      <td>2024-09-20T14:34:12.950537</td>\n",
       "      <td>17.431519</td>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>3.2.1 Scaled Dot-Product Attention\\nWhile for ...</td>\n",
       "      <td>$.main-text[49]</td>\n",
       "      <td>4</td>\n",
       "      <td>[107.04788208, 174.61242676, 504.51269531, 219...</td>\n",
       "      <td>a1172fbaaef42850d7c9340740acd47badd2ce11b6f20a...</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>C. Evaluation\\n1) Academic Benchmarks: We eval...</td>\n",
       "      <td>$.main-text[396]</td>\n",
       "      <td>16</td>\n",
       "      <td>[311.16088867, 43.3947525, 563.29870605, 89.57...</td>\n",
       "      <td>c92d1e14b79373b58950b72ad254fb01104403c6abb214...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>REFERENCES\\n[1] J. Wei, M. Bosma, V. Y. Zhao, ...</td>\n",
       "      <td>$.main-text[250]</td>\n",
       "      <td>13</td>\n",
       "      <td>[145.87217712, 195.048172, 202.44746399, 204.7...</td>\n",
       "      <td>31b2b97050b77f7dfa0c1b0050fdb5f53c41e7ff45b53b...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  num_pages  num_tables  num_doc_elements  \\\n",
       "28   attention-is-all-you-need.pdf         15           4               193   \n",
       "238  Granite_Foundation_Models.pdf         20          13               444   \n",
       "217  Granite_Foundation_Models.pdf         20          13               444   \n",
       "\n",
       "                              document_id  ext  \\\n",
       "28   802bded3-330b-48d7-bb52-1098500decec  pdf   \n",
       "238  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "217  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "28   414d95bff49753a945e0917d47b840bef75c67bd56af13...  137574   \n",
       "238  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "217  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "\n",
       "                  date_acquired  pdf_convert_time  \\\n",
       "28   2024-09-20T14:34:12.950537         17.431519   \n",
       "238  2024-09-20T14:33:55.489893         29.919255   \n",
       "217  2024-09-20T14:33:55.489893         29.919255   \n",
       "\n",
       "                   source_filename  \\\n",
       "28   attention-is-all-you-need.pdf   \n",
       "238  Granite_Foundation_Models.pdf   \n",
       "217  Granite_Foundation_Models.pdf   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "28   3.2.1 Scaled Dot-Product Attention\\nWhile for ...   $.main-text[49]   \n",
       "238  C. Evaluation\\n1) Academic Benchmarks: We eval...  $.main-text[396]   \n",
       "217  REFERENCES\\n[1] J. Wei, M. Bosma, V. Y. Zhao, ...  $.main-text[250]   \n",
       "\n",
       "     page_number                                               bbox  \\\n",
       "28             4  [107.04788208, 174.61242676, 504.51269531, 219...   \n",
       "238           16  [311.16088867, 43.3947525, 563.29870605, 89.57...   \n",
       "217           13  [145.87217712, 195.048172, 202.44746399, 204.7...   \n",
       "\n",
       "                                            chunk_hash  chunk_id  \n",
       "28   a1172fbaaef42850d7c9340740acd47badd2ce11b6f20a...       206  \n",
       "238  c92d1e14b79373b58950b72ad254fb01104403c6abb214...       150  \n",
       "217  31b2b97050b77f7dfa0c1b0050fdb5f53c41e7ff45b53b...       129  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "## Step-5: Exact Dedup\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### 5.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c7a1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-4: Processing input='output-papers/03_docid_out' --> output='output-papers/04_exact_dedupe_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  = 4\n",
    "\n",
    "input_folder = output_docid_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_exact_dedupe_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### 5.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34:14 INFO - exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'chunk_hash', 'use_snapshot': False, 'snapshot_directory': None}\n",
      "14:34:14 INFO - pipeline id pipeline_id\n",
      "14:34:14 INFO - code location None\n",
      "14:34:14 INFO - data factory data_ is using local data access: input_folder - output-papers/03_docid_out output_folder - output-papers/04_exact_dedupe_out\n",
      "14:34:14 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:34:14 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "14:34:14 INFO - orchestrator ededup started at 2024-09-20 14:34:14\n",
      "14:34:14 INFO - Number of files is 2, source profile {'max_file_size': 0.053391456604003906, 'min_file_size': 0.02887439727783203, 'total_file_size': 0.08226585388183594}\n",
      "14:34:14 INFO - Starting from the beginning\n",
      "14:34:14 INFO - Completed 1 files (50.0%) in 0.0 min\n",
      "14:34:14 INFO - Completed 2 files (100.0%) in 0.0 min\n",
      "14:34:14 INFO - Done processing 2 files, waiting for flush() completion.\n",
      "14:34:14 INFO - done flushing in 0.0 sec\n",
      "14:34:14 INFO - Completed execution in 0.0 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:4 completed successfully\n",
      "CPU times: user 19.2 ms, sys: 1.88 ms, total: 21.1 ms\n",
      "Wall time: 17.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import ededup transform configuration\n",
    "from ededup_transform_python import EdedupPythonTransformRuntimeConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # ededup parameters\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "    \"ededup_doc_id_column\": \"chunk_hash\",\n",
    "    \n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = PythonTransformLauncher(EdedupPythonTransformRuntimeConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1c3c3",
   "metadata": {},
   "source": [
    "### 5.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d824ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (266, 17)\n",
      "Output data dimensions (rows x columns)=  (266, 18)\n",
      "Input chunks before exact dedupe : 266\n",
      "Output chunks after exact dedupe : 266\n",
      "Duplicate chunks removed :   0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>chunk_hash</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>A. Model Outputs from Safety Tasks\\nDisclaimer...</td>\n",
       "      <td>$.main-text[376]</td>\n",
       "      <td>16</td>\n",
       "      <td>[48.15233612, 488.89440918, 300.33282471, 547....</td>\n",
       "      <td>dc8577aca3ceb1e285464d661766a434f6fe8d1872d4c0...</td>\n",
       "      <td>137</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>C. Energy Consumption and Carbon Emissions\\nTh...</td>\n",
       "      <td>$.main-text[132]</td>\n",
       "      <td>6</td>\n",
       "      <td>[48.11833191, 383.87985229, 300.37054443, 478....</td>\n",
       "      <td>0a534e6e106839971a4b33caf6cfad192a3636cc246df9...</td>\n",
       "      <td>56</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>A. Algorithmic Details\\nThe granite.13b.v1 bas...</td>\n",
       "      <td>$.main-text[107]</td>\n",
       "      <td>4</td>\n",
       "      <td>[311.14950562, 43.96509933, 563.44586182, 65.6...</td>\n",
       "      <td>9fe2656e1b1b1057b9579fa94536926c8f07752ef37fc5...</td>\n",
       "      <td>43</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  num_pages  num_tables  num_doc_elements  \\\n",
       "225  Granite_Foundation_Models.pdf         20          13               444   \n",
       "144  Granite_Foundation_Models.pdf         20          13               444   \n",
       "131  Granite_Foundation_Models.pdf         20          13               444   \n",
       "\n",
       "                              document_id  ext  \\\n",
       "225  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "144  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "131  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "225  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "144  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "131  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "\n",
       "                  date_acquired  pdf_convert_time  \\\n",
       "225  2024-09-20T14:33:55.489893         29.919255   \n",
       "144  2024-09-20T14:33:55.489893         29.919255   \n",
       "131  2024-09-20T14:33:55.489893         29.919255   \n",
       "\n",
       "                   source_filename  \\\n",
       "225  Granite_Foundation_Models.pdf   \n",
       "144  Granite_Foundation_Models.pdf   \n",
       "131  Granite_Foundation_Models.pdf   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "225  A. Model Outputs from Safety Tasks\\nDisclaimer...  $.main-text[376]   \n",
       "144  C. Energy Consumption and Carbon Emissions\\nTh...  $.main-text[132]   \n",
       "131  A. Algorithmic Details\\nThe granite.13b.v1 bas...  $.main-text[107]   \n",
       "\n",
       "     page_number                                               bbox  \\\n",
       "225           16  [48.15233612, 488.89440918, 300.33282471, 547....   \n",
       "144            6  [48.11833191, 383.87985229, 300.37054443, 478....   \n",
       "131            4  [311.14950562, 43.96509933, 563.44586182, 65.6...   \n",
       "\n",
       "                                            chunk_hash  chunk_id removed  \n",
       "225  dc8577aca3ceb1e285464d661766a434f6fe8d1872d4c0...       137      []  \n",
       "144  0a534e6e106839971a4b33caf6cfad192a3636cc246df9...        56      []  \n",
       "131  9fe2656e1b1b1057b9579fa94536926c8f07752ef37fc5...        43      []  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (f\"Input chunks before exact dedupe : {input_df.shape[0]:,}\")\n",
    "print (f\"Output chunks after exact dedupe : {output_df.shape[0]:,}\")\n",
    "print (\"Duplicate chunks removed :  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## Step-6: Fuzzy Dedup\n",
    "\n",
    "**Fuzzy dedupe is currently available in RAY version**\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### 6.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-5: Processing input='output-papers/04_exact_dedupe_out' --> output='output-papers/05_fuzzy_dedupe_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  = 5\n",
    "\n",
    "input_folder = output_exact_dedupe_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_fuzzy_dedupe_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### 6.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34:14 INFO - Running locally\n",
      "14:34:14 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'chunk_id', 'cluster_column': 'chunk_hash', 'bucket_cpu': 0.3, 'mhash_cpu': 0.3, 'doc_cpu': 0.3, 'num_doc_actors': 1, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 1, 'num_permutations': 64, 'threshold': 0.7, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 1}}\n",
      "14:34:14 INFO - data factory data_ is using local data access: input_folder - output-papers/04_exact_dedupe_out output_folder - output-papers/05_fuzzy_dedupe_out\n",
      "14:34:14 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:34:14 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "14:34:14 INFO - pipeline id pipeline_id\n",
      "14:34:14 INFO - code location None\n",
      "14:34:14 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "14:34:14 INFO - actor creation delay 0\n",
      "14:34:14 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-09-20 14:34:18,123\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - orchestrator started at 2024-09-20 14:34:19\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - Number of files is 2, source profile {'max_file_size': 0.05375099182128906, 'min_file_size': 0.029233932495117188, 'total_file_size': 0.08298492431640625}\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - Cluster resources: {'cpus': 16, 'gpus': 1, 'memory': 7.936802674084902, 'object_store': 3.9684013361111283}\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - Fuzzy: num buckets 8, bucket length 8\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - Table preprocessing uses 1 readers\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:19 INFO - created 1 table processor actors\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:20 INFO - Completed 1 files in 0.015 min\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:20 INFO - Completed 1 files (50.0%)  in 0.015 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:20 INFO - Completed processing 2 files in 0.016 min\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:20 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:21 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:21 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:22 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:22 INFO - created 1 document actors\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:22 INFO - created 1 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:22 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:22 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=3988839)\u001b[0m 14:34:22 INFO - processing buckets 0 long, 2126 short\n",
      "\u001b[36m(BucketsHash pid=3988839)\u001b[0m 14:34:22 INFO - Done submitting long buckets\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:22 INFO - Done processing buckets in 0.011 min\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:22 INFO - creating document snapshots\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=3989148)\u001b[0m 14:34:22 INFO - Waiting bucket processing completion. Submitted requests 22\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:23 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:23 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:26 INFO - Completed processing 2 files in 0.047 min\n",
      "\u001b[36m(orchestrate pid=3988010)\u001b[0m 14:34:26 INFO - done flushing in 0.001 sec\n",
      "14:34:36 INFO - Completed execution in 0.364 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:5 completed successfully\n",
      "CPU times: user 465 ms, sys: 348 ms, total: 813 ms\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"chunk_id\",\n",
    "    \"fdedup_cluster_column\": \"chunk_hash\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.3,\n",
    "    \"fdedup_doc_cpu\": 0.3,\n",
    "    \"fdedup_mhash_cpu\": 0.3,\n",
    "    \"fdedup_num_doc_actors\": 1,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 1,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.7, # between 0.0 to 1.0 ; smaller values tend to be more lenient in finding near dupes; close to 1.0 is more strict\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8cd11",
   "metadata": {},
   "source": [
    "### 6.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e899ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (266, 18)\n",
      "Output data dimensions (rows x columns)=  (266, 18)\n",
      "Duplicate chunks removed  by fuzzy-dedupe:   0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>removed</th>\n",
       "      <th>chunk_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>JAPANESE FINANCE BENCHMARK EVALUATION RESULTS\\...</td>\n",
       "      <td>$.main-text[432]</td>\n",
       "      <td>18</td>\n",
       "      <td>[213.05499268, 244.48545837, 398.37347412, 261...</td>\n",
       "      <td>170</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>B. Granite Model Evaluation and Comparison\\nEv...</td>\n",
       "      <td>$.main-text[180]</td>\n",
       "      <td>8</td>\n",
       "      <td>[48.16522598, 108.47112274, 300.1288147, 130.1...</td>\n",
       "      <td>84</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>A. Data Clearance and Acquisition\\nFig. 3. IBM...</td>\n",
       "      <td>$.main-text[60]</td>\n",
       "      <td>3</td>\n",
       "      <td>[311.41522217, 586.98126221, 457.3286438, 595....</td>\n",
       "      <td>25</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  num_pages  num_tables  num_doc_elements  \\\n",
       "258  Granite_Foundation_Models.pdf         20          13               444   \n",
       "172  Granite_Foundation_Models.pdf         20          13               444   \n",
       "113  Granite_Foundation_Models.pdf         20          13               444   \n",
       "\n",
       "                              document_id  ext  \\\n",
       "258  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "172  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "113  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "258  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "172  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "113  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "\n",
       "                  date_acquired  pdf_convert_time  \\\n",
       "258  2024-09-20T14:33:55.489893         29.919255   \n",
       "172  2024-09-20T14:33:55.489893         29.919255   \n",
       "113  2024-09-20T14:33:55.489893         29.919255   \n",
       "\n",
       "                   source_filename  \\\n",
       "258  Granite_Foundation_Models.pdf   \n",
       "172  Granite_Foundation_Models.pdf   \n",
       "113  Granite_Foundation_Models.pdf   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "258  JAPANESE FINANCE BENCHMARK EVALUATION RESULTS\\...  $.main-text[432]   \n",
       "172  B. Granite Model Evaluation and Comparison\\nEv...  $.main-text[180]   \n",
       "113  A. Data Clearance and Acquisition\\nFig. 3. IBM...   $.main-text[60]   \n",
       "\n",
       "     page_number                                               bbox  chunk_id  \\\n",
       "258           18  [213.05499268, 244.48545837, 398.37347412, 261...       170   \n",
       "172            8  [48.16522598, 108.47112274, 300.1288147, 130.1...        84   \n",
       "113            3  [311.41522217, 586.98126221, 457.3286438, 595....        25   \n",
       "\n",
       "    removed  chunk_hash  \n",
       "258      []          -1  \n",
       "172      []          -1  \n",
       "113      []          -1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (\"Duplicate chunks removed  by fuzzy-dedupe:  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## Step-7:   Text encoding\n",
    "\n",
    "Encode text for the vector storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd33b1",
   "metadata": {},
   "source": [
    "### 7.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-6: Processing input='output-papers/05_fuzzy_dedupe_out' --> output='output-papers/06_embeddings_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  = 6\n",
    "\n",
    "input_folder = output_fuzzy_dedupe_dir\n",
    "output_folder =  output_embeddings_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9112479",
   "metadata": {},
   "source": [
    "### 7.2 - Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34:38 INFO - text_encoder parameters are : {'content_column_name': 'contents', 'output_embeddings_column_name': 'embeddings', 'model_name': 'sentence-transformers/all-MiniLM-L6-v2'}\n",
      "14:34:38 INFO - pipeline id pipeline_id\n",
      "14:34:38 INFO - code location None\n",
      "14:34:38 INFO - data factory data_ is using local data access: input_folder - output-papers/05_fuzzy_dedupe_out output_folder - output-papers/06_embeddings_out\n",
      "14:34:38 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:34:38 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "14:34:38 INFO - orchestrator text_encoder started at 2024-09-20 14:34:38\n",
      "14:34:38 INFO - Number of files is 2, source profile {'max_file_size': 0.04749584197998047, 'min_file_size': 0.02600860595703125, 'total_file_size': 0.07350444793701172}\n",
      "14:34:41 INFO - Completed 1 files (50.0%) in 0.016 min\n",
      "14:34:42 INFO - Completed 2 files (100.0%) in 0.023 min\n",
      "14:34:42 INFO - Done processing 2 files, waiting for flush() completion.\n",
      "14:34:42 INFO - done flushing in 0.0 sec\n",
      "14:34:42 INFO - Completed execution in 0.06 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:6 completed successfully\n",
      "CPU times: user 1.94 s, sys: 220 ms, total: 2.16 s\n",
      "Wall time: 4.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from text_encoder_transform_python import TextEncoderPythonTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "params = {\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # text_encoder\n",
    "    \"text_encoder_model_name\": MY_CONFIG.EMBEDDING_MODEL,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = PythonTransformLauncher(TextEncoderPythonTransformConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734852c",
   "metadata": {},
   "source": [
    "### 7.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b1c1d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (266, 18)\n",
      "Output data dimensions (rows x columns)=  (266, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>removed</th>\n",
       "      <th>chunk_hash</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>7a91936e-f7b4-4d90-9278-9ec1d2a19475</td>\n",
       "      <td>pdf</td>\n",
       "      <td>3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...</td>\n",
       "      <td>394434</td>\n",
       "      <td>2024-09-20T14:33:55.489893</td>\n",
       "      <td>29.919255</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>B. Downstream Documentation\\nFor downstream us...</td>\n",
       "      <td>$.main-text[236]</td>\n",
       "      <td>11</td>\n",
       "      <td>[311.23956299, 62.55028915, 563.27984619, 84.1...</td>\n",
       "      <td>122</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.06926964, -0.09363627, 0.01313181, 0.01689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>802bded3-330b-48d7-bb52-1098500decec</td>\n",
       "      <td>pdf</td>\n",
       "      <td>414d95bff49753a945e0917d47b840bef75c67bd56af13...</td>\n",
       "      <td>137574</td>\n",
       "      <td>2024-09-20T14:34:12.950537</td>\n",
       "      <td>17.431519</td>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>6.1 Machine Translation\\nOn the WMT 2014 Engli...</td>\n",
       "      <td>$.main-text[108]</td>\n",
       "      <td>8</td>\n",
       "      <td>[107.27262115, 260.13467407, 505.24533081, 302...</td>\n",
       "      <td>240</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.03620785, -0.04418195, 0.02379151, -0.0163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>802bded3-330b-48d7-bb52-1098500decec</td>\n",
       "      <td>pdf</td>\n",
       "      <td>414d95bff49753a945e0917d47b840bef75c67bd56af13...</td>\n",
       "      <td>137574</td>\n",
       "      <td>2024-09-20T14:34:12.950537</td>\n",
       "      <td>17.431519</td>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>5.3 Optimizer\\nWe used the Adam optimizer [20]...</td>\n",
       "      <td>$.main-text[95]</td>\n",
       "      <td>7</td>\n",
       "      <td>[107.17208862, 193.10830688, 504.16403198, 215...</td>\n",
       "      <td>232</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.037252318, -0.0319632, 0.0018505336, -0.037...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  num_pages  num_tables  num_doc_elements  \\\n",
       "210  Granite_Foundation_Models.pdf         20          13               444   \n",
       "62   attention-is-all-you-need.pdf         15           4               193   \n",
       "54   attention-is-all-you-need.pdf         15           4               193   \n",
       "\n",
       "                              document_id  ext  \\\n",
       "210  7a91936e-f7b4-4d90-9278-9ec1d2a19475  pdf   \n",
       "62   802bded3-330b-48d7-bb52-1098500decec  pdf   \n",
       "54   802bded3-330b-48d7-bb52-1098500decec  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "210  3fdea13fde26d449edec114b8a7b70c4565673aa90c86b...  394434   \n",
       "62   414d95bff49753a945e0917d47b840bef75c67bd56af13...  137574   \n",
       "54   414d95bff49753a945e0917d47b840bef75c67bd56af13...  137574   \n",
       "\n",
       "                  date_acquired  pdf_convert_time  \\\n",
       "210  2024-09-20T14:33:55.489893         29.919255   \n",
       "62   2024-09-20T14:34:12.950537         17.431519   \n",
       "54   2024-09-20T14:34:12.950537         17.431519   \n",
       "\n",
       "                   source_filename  \\\n",
       "210  Granite_Foundation_Models.pdf   \n",
       "62   attention-is-all-you-need.pdf   \n",
       "54   attention-is-all-you-need.pdf   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "210  B. Downstream Documentation\\nFor downstream us...  $.main-text[236]   \n",
       "62   6.1 Machine Translation\\nOn the WMT 2014 Engli...  $.main-text[108]   \n",
       "54   5.3 Optimizer\\nWe used the Adam optimizer [20]...   $.main-text[95]   \n",
       "\n",
       "     page_number                                               bbox  chunk_id  \\\n",
       "210           11  [311.23956299, 62.55028915, 563.27984619, 84.1...       122   \n",
       "62             8  [107.27262115, 260.13467407, 505.24533081, 302...       240   \n",
       "54             7  [107.17208862, 193.10830688, 504.16403198, 215...       232   \n",
       "\n",
       "    removed  chunk_hash                                         embeddings  \n",
       "210      []          -1  [-0.06926964, -0.09363627, 0.01313181, 0.01689...  \n",
       "62       []          -1  [-0.03620785, -0.04418195, 0.02379151, -0.0163...  \n",
       "54       []          -1  [0.037252318, -0.0319632, 0.0018505336, -0.037...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12630-be6b-4188-a925-77117155617b",
   "metadata": {},
   "source": [
    "## Step-8: Copy output to final output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Copied output from 'output-papers/06_embeddings_out' --> 'output-papers/output_final'\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER_FINAL, ignore_errors=True)\n",
    "shutil.copytree(src=output_folder, dst=MY_CONFIG.OUTPUT_FOLDER_FINAL)\n",
    "\n",
    "print (f\"‚úÖ Copied output from '{output_folder}' --> '{MY_CONFIG.OUTPUT_FOLDER_FINAL}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce85f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
