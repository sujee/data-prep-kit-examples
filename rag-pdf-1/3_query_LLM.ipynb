{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Data using LLM\n",
    "\n",
    "Here is the overall RAG pipeline.   In this notebook, we will do steps (6), (7), (8), (9) and (10)\n",
    "- Importing data is already done in this notebook [rag_2_load_data_into_milvus.ipynb](rag_2_load_data_into_milvus.ipynb)\n",
    "- ðŸ‘‰ Step 6: Calculate embedding for user query\n",
    "- ðŸ‘‰ Step 7 & 8: Send the query to vector db to retrieve relevant documents\n",
    "- ðŸ‘‰ Step 9 & 10: Send the query and relevant documents (returned above step) to LLM and get answers to our query\n",
    "\n",
    "![image missing](media/rag-overview-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Connect to Vector Database\n",
    "\n",
    "Milvus can be embedded and easy to use.\n",
    "\n",
    "<span style=\"color:blue;\">Note: If you encounter an error about unable to load database, try this: </span>\n",
    "\n",
    "- <span style=\"color:blue;\">In **vscode** : **restart the kernel** of previous notebook. This will release the db.lock </span>\n",
    "- <span style=\"color:blue;\">In **Jupyter**: Do `File --> Close and Shutdown Notebook` of previous notebook. This will release the db.lock</span>\n",
    "- <span style=\"color:blue;\">Re-run this cell again</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/my-stuff/ai-alliance/data-prep-kit-examples/dpk-dev/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/sujee/my-stuff/ai-alliance/data-prep-kit-examples/dpk-dev/.venv/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to Milvus instance:  ./rag_1_dpk.db\n"
     ]
    }
   ],
   "source": [
    "# connect to vector db\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri = MY_CONFIG.DB_URI ,\n",
    "    dim = MY_CONFIG.EMBEDDING_LENGTH , \n",
    "    collection_name = MY_CONFIG.COLLECTION_NAME,\n",
    "    overwrite=False  # so we load the index from db\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "print (\"âœ… Connected to Milvus instance: \", MY_CONFIG.DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Setup Embeddings\n",
    "\n",
    "Use the same embeddings we used to index our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using embedding model: nebius/Qwen/Qwen3-Embedding-8B\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.litellm import LiteLLMEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = LiteLLMEmbedding(\n",
    "        model_name=MY_CONFIG.EMBEDDING_MODEL,\n",
    "        embed_batch_size=50,  # Batch size for embedding (default is 10)\n",
    "    )\n",
    "print (f\"âœ… Using embedding model: {MY_CONFIG.EMBEDDING_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## local embedding model\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.core import Settings\n",
    "\n",
    "# print (\"âœ… Using embedding Model:\", MY_CONFIG.EMBEDDING_MODEL)\n",
    "# print (\"âœ… Using embedding length:\", MY_CONFIG.EMBEDDING_LENGTH)\n",
    "\n",
    "# Settings.embed_model = HuggingFaceEmbedding(\n",
    "#     model_name = MY_CONFIG.EMBEDDING_MODEL\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Load Document Index from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded index from vector db: ./rag_1_dpk.db\n",
      "CPU times: user 757 Î¼s, sys: 0 ns, total: 757 Î¼s\n",
      "Wall time: 734 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, storage_context=storage_context)\n",
    "\n",
    "print (\"âœ… Loaded index from vector db:\", MY_CONFIG.DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Using LLM\n",
    "\n",
    "We can use LLMs running on remote services or locally (e.g. using Ollama).  We use [LiteLLM library](https://docs.litellm.ai/docs/) to choose LLM runtime.\n",
    "\n",
    "Here are some examples.\n",
    "\n",
    "- [Nebuis Token Factory](https://tokenfactory.nebius.com/)\n",
    "- [replicate.com](https://replicate.com)\n",
    "\n",
    "**How to use the LLM inference services**\n",
    "\n",
    "If Using Nebius\n",
    "\n",
    "Update `.env` file as follows\n",
    "\n",
    "```ini\n",
    "LLM_MODEL = 'nebius/openai/gpt-oss-120b'\n",
    "NEBIUS_API_KEY = 'your key goes here'\n",
    "```\n",
    "\n",
    "If using Replicate\n",
    "\n",
    "```ini\n",
    "LLM_MODEL = 'ibm-granite/granite-3.3-8b-instruct'\n",
    "REPLICATE_API_TOKEN=xyz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using LLM model : nebius/openai/gpt-oss-120b\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.litellm import LiteLLM\n",
    "\n",
    "# Setup LLM\n",
    "print (f\"âœ… Using LLM model : {MY_CONFIG.LLM_MODEL}\")\n",
    "Settings.llm = LiteLLM (\n",
    "        model=MY_CONFIG.LLM_MODEL,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-7: Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granite models were built using a comprehensive pipeline that began with largeâ€‘scale data collection, followed by careful filtering and preprocessing of the codeâ€‘related corpora. The resulting datasets were fed into decoderâ€‘only transformer architectures ranging from 3â€¯billion to 34â€¯billion parameters. Training proceeded in multiple stages: an initial preâ€‘training phase on the curated code data, then a subsequent instructionâ€‘tuning stage that exposed the models to taskâ€‘specific prompts for generation, bug fixing, explanation, and documentation. Throughout, the training process incorporated the architectural and hyperâ€‘parameter choices detailed in the model design section, and the entire workflow was documented across the data preparation, model architecture, training, and instructionâ€‘tuning sections of the paper.\n",
      "CPU times: user 110 ms, sys: 14.2 ms, total: 124 ms\n",
      "Wall time: 7.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import query_utils\n",
    "\n",
    "question = \"How were Granite models trained?\"\n",
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query(question, MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention mechanism takes a query vector together with a collection of keyâ€‘value pairs (each also represented as vectors) and produces an output vector. It does this by computing similarity scores between the query and each key, turning those scores into weights, and then forming a weighted sum of the corresponding values. The resulting weighted sum is the attention output.\n",
      "CPU times: user 35.5 ms, sys: 7.33 ms, total: 42.8 ms\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import query_utils\n",
    "\n",
    "question = \"What is attention mechanism?\"\n",
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query(question, MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain information about the date of the moon landing.\n",
      "CPU times: user 33.2 ms, sys: 4.4 ms, total: 37.6 ms\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import query_utils\n",
    "\n",
    "question = \"When was the moon landing?\"\n",
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query(question, MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpk-example-1",
   "language": "python",
   "name": "dpk-example-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
