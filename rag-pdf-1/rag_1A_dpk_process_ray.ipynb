{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Data Processing for RAG with Data Prep Kit (RAY)</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15976e3",
   "metadata": {},
   "source": [
    "## Before Running the notebook\n",
    "\n",
    "Please complete [setting up python dev environment](./setup-python-dev-env.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will process PDF documents as part of RAG pipeline\n",
    "\n",
    "![](media/rag-overview-2.png)\n",
    "\n",
    "This notebook will perform steps 1, 2 and 3 in RAG pipeline.\n",
    "\n",
    "Here are the processing steps:\n",
    "\n",
    "- **pdf2parquet** : Extract text from PDF and convert them into parquet files\n",
    "- **Chunk documents**: Split the PDFs into 'meaningful sections' (paragraphs, sentences ..etc)\n",
    "- **Doc_ID generation**: Each chunk is assigned a uniq id, based on content and hash\n",
    "- **Exact Dedup**: Chunks with exact same content are filtered out\n",
    "- **Fuzzy Dedup**: Eliminate chunks that are 'very similar' content\n",
    "- **Doc quality**: Scores the documents based on criteria like number of words, if it contains bad words ..etc\n",
    "- **Text encoder**: Convert chunks into vectors using embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b10be1",
   "metadata": {},
   "source": [
    "## Step-1: Configuration\n",
    "\n",
    "### 1.1 - Common Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33345487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc3f0e",
   "metadata": {},
   "source": [
    "### 1.2 - Inspect Input Data\n",
    "\n",
    "We have a bnunch of datasets in [data](../data) folder.  Examine them\n",
    "\n",
    "We will use one of them or feel free to bring your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### 1.3  - Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleared output directory\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(MY_CONFIG.INPUT_DATA_DIR ):\n",
    "    raise Exception (f\"‚ùå Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
    "\n",
    "output_parquet_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '01_parquet_out')\n",
    "output_chunk_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '02_chunk_out')\n",
    "output_docid_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '03_docid_out')\n",
    "output_exact_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '04_exact_dedupe_out')\n",
    "output_fuzzy_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '05_fuzzy_dedupe_out')\n",
    "output_embeddings_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '06_embeddings_out')\n",
    "\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print (\"‚úÖ Cleared output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d976e-cb4c-4469-af39-4b7ea507e9d8",
   "metadata": {},
   "source": [
    "### 1.4 - Import Common python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66178913-42b8-426b-a2e9-9587268fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "from data_processing.utils import ParamsUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "<a id=\"pdf2parquet\"></a>\n",
    "\n",
    "## Step-2: pdf2parquet -  Convert data from PDF to Parquet\n",
    "\n",
    "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
    "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### 2.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-1: Processing input='../data/papers' --> output='output-papers/01_parquet_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE = 1 \n",
    "\n",
    "input_folder = MY_CONFIG.INPUT_DATA_DIR\n",
    "output_folder =  output_parquet_dir\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### 2.2 -  Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:35:40 INFO - Running locally\n",
      "14:35:40 INFO - pdf2parquet parameters are : {'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.JSON: 'application/json'>, 'do_table_structure': True, 'do_ocr': True, 'double_precision': 8}\n",
      "14:35:40 INFO - data factory data_ is using local data access: input_folder - ../data/papers output_folder - output-papers/01_parquet_out\n",
      "14:35:40 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:35:40 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "14:35:40 INFO - pipeline id pipeline_id\n",
      "14:35:40 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'}\n",
      "14:35:40 INFO - number of workers 2 worker options {'num_cpus': 1, 'memory': 2147483648, 'max_restarts': -1}\n",
      "14:35:40 INFO - actor creation delay 0\n",
      "14:35:40 INFO - job details {'job category': 'preprocessing', 'job name': 'pdf2parquet', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-09-20 14:35:42,326\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=3990919)\u001b[0m 14:35:45 INFO - orchestrator started at 2024-09-20 14:35:45\n",
      "\u001b[36m(orchestrate pid=3990919)\u001b[0m 14:35:45 INFO - Number of files is 2, source profile {'max_file_size': 5.398899078369141, 'min_file_size': 2.112621307373047, 'total_file_size': 7.5115203857421875}\n",
      "\u001b[36m(orchestrate pid=3990919)\u001b[0m 14:35:45 INFO - Cluster resources: {'cpus': 16, 'gpus': 1, 'memory': 8.430704499594867, 'object_store': 4.21535224840045}\n",
      "\u001b[36m(orchestrate pid=3990919)\u001b[0m 14:35:45 INFO - Number of workers - 2 with {'num_cpus': 1, 'memory': 2147483648, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=3990919)\u001b[0m 14:35:45 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(RayTransformFileProcessor pid=3991792)\u001b[0m 14:35:48 INFO - Initializing models\n",
      "Fetching 9 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 18715.29it/s]\n",
      "\u001b[36m(RayTransformFileProcessor pid=3991792)\u001b[0m Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "\u001b[36m(orchestrate pid=3990919)\u001b[0m 14:37:49 INFO - Completed processing 2 files in 2.057 min\n",
      "\u001b[36m(orchestrate pid=3990919)\u001b[0m 14:37:49 INFO - done flushing in 0.001 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=3991793)\u001b[0m 14:35:48 INFO - Initializing models\n",
      "Fetching 9 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 46147.60it/s]\n",
      "\u001b[36m(RayTransformFileProcessor pid=3991793)\u001b[0m Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "14:37:59 INFO - Completed execution in 2.313 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:1 completed successfully\n",
      "CPU times: user 3.66 s, sys: 573 ms, total: 4.23 s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pdf2parquet_transform import (\n",
    "    pdf2parquet_contents_type_cli_param,\n",
    "    pdf2parquet_contents_types,\n",
    ")\n",
    "from pdf2parquet_transform_python import Pdf2ParquetPythonTransformConfiguration\n",
    "from pdf2parquet_transform_ray import Pdf2ParquetRayTransformConfiguration\n",
    "\n",
    "from data_processing.utils import GB, ParamsUtils\n",
    "\n",
    "\n",
    "# create parameters\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS, \"memory\": MY_CONFIG.RAY_MEMORY_GB * GB}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "ingest_config = {\n",
    "    pdf2parquet_contents_type_cli_param: pdf2parquet_contents_types.JSON,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    \"data_files_to_use\": ast.literal_eval(\"['.pdf']\"),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(Pdf2ParquetRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca790e0",
   "metadata": {},
   "source": [
    "### 2.3 - Inspect Generated output\n",
    "\n",
    "Here we should see one entry per input file processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe59563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensions (rows x columns)=  (2, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>82b50c62-ad6d-4c6e-94f5-128d5c8141dc</td>\n",
       "      <td>pdf</td>\n",
       "      <td>414d95bff49753a945e0917d47b840bef75c67bd56af13...</td>\n",
       "      <td>137574</td>\n",
       "      <td>2024-09-20T14:36:45.815652</td>\n",
       "      <td>53.907655</td>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        filename  \\\n",
       "0  attention-is-all-you-need.pdf   \n",
       "1  Granite_Foundation_Models.pdf   \n",
       "\n",
       "                                            contents  num_pages  num_tables  \\\n",
       "0  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...         15           4   \n",
       "1  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...         20          13   \n",
       "\n",
       "   num_doc_elements                           document_id  ext  \\\n",
       "0               193  82b50c62-ad6d-4c6e-94f5-128d5c8141dc  pdf   \n",
       "1               444  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "\n",
       "                                                hash    size  \\\n",
       "0  414d95bff49753a945e0917d47b840bef75c67bd56af13...  137574   \n",
       "1  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "\n",
       "                date_acquired  pdf_convert_time                source_filename  \n",
       "0  2024-09-20T14:36:45.815652         53.907655  attention-is-all-you-need.pdf  \n",
       "1  2024-09-20T14:37:49.189377        117.302547  Granite_Foundation_Models.pdf  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Output dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.head(5)\n",
    "\n",
    "## To display certain columns\n",
    "#parquet_df[['column1', 'column2', 'column3']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72274586",
   "metadata": {},
   "source": [
    "<a id=\"chunking\"></a>\n",
    "\n",
    "##  Step-3: Doc chunks\n",
    "\n",
    "Split the documents in chunks, according to their layout segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96198fa6",
   "metadata": {},
   "source": [
    "### 3.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305f00a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-2: Processing input='output-papers/01_parquet_out' --> output='output-papers/02_chunk_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE = 2\n",
    "\n",
    "input_folder = output_parquet_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_chunk_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f2cd1",
   "metadata": {},
   "source": [
    "### 3.2 -  Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7b18d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:38:01 INFO - Running locally\n",
      "14:38:01 INFO - doc_chunk parameters are : {'chunking_type': <chunking_types.DL_JSON: 'dl_json'>, 'content_column_name': 'contents', 'output_chunk_column_name': 'contents', 'output_jsonpath_column_name': 'doc_jsonpath', 'output_pageno_column_name': 'page_number', 'output_bbox_column_name': 'bbox'}\n",
      "14:38:01 INFO - data factory data_ is using local data access: input_folder - output-papers/01_parquet_out output_folder - output-papers/02_chunk_out\n",
      "14:38:01 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:38:01 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "14:38:01 INFO - pipeline id pipeline_id\n",
      "14:38:01 INFO - code location None\n",
      "14:38:01 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "14:38:01 INFO - actor creation delay 0\n",
      "14:38:01 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_chunk', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-09-20 14:38:03,391\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=3994290)\u001b[0m 14:38:05 INFO - orchestrator started at 2024-09-20 14:38:05\n",
      "\u001b[36m(orchestrate pid=3994290)\u001b[0m 14:38:05 INFO - Number of files is 2, source profile {'max_file_size': 0.08796119689941406, 'min_file_size': 0.03559398651123047, 'total_file_size': 0.12355518341064453}\n",
      "\u001b[36m(orchestrate pid=3994290)\u001b[0m 14:38:05 INFO - Cluster resources: {'cpus': 16, 'gpus': 1, 'memory': 8.447726441547275, 'object_store': 4.223863219842315}\n",
      "\u001b[36m(orchestrate pid=3994290)\u001b[0m 14:38:05 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=3994290)\u001b[0m 14:38:05 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=3994290)\u001b[0m 14:38:07 INFO - Completed processing 2 files in 0.032 min\n",
      "\u001b[36m(orchestrate pid=3994290)\u001b[0m 14:38:07 INFO - done flushing in 0.001 sec\n",
      "14:38:17 INFO - Completed execution in 0.27 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:2 completed successfully\n",
      "CPU times: user 947 ms, sys: 278 ms, total: 1.23 s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Import doc_json_chunk transform configuration\n",
    "from doc_chunk_transform_ray import DocChunkRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # doc_chunk arguments\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(DocChunkRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213afdf6",
   "metadata": {},
   "source": [
    "### 3.3 - Inspect Generated output\n",
    "\n",
    "We would see documents are split into many chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8138d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed : 2\n",
      "Chunks created : 266\n",
      "Input data dimensions (rows x columns)=  (2, 12)\n",
      "Output data dimensions (rows x columns)=  (266, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>A. Tokenization\\nWe use the byte-level BPE tok...</td>\n",
       "      <td>$.main-text[402]</td>\n",
       "      <td>17</td>\n",
       "      <td>[48.14419556, 380.41003418, 300.07315063, 414....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>A. Foundation Model Evaluation Framework\\n4) M...</td>\n",
       "      <td>$.main-text[164]</td>\n",
       "      <td>7</td>\n",
       "      <td>[48.16139603, 64.27197266, 300.24572754, 98.24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>JAPANESE FINANCE BENCHMARK EVALUATION RESULTS\\...</td>\n",
       "      <td>$.tables[11]</td>\n",
       "      <td>19</td>\n",
       "      <td>[47.96198654, 486.60632324, 587.04760742, 698....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  num_pages  num_tables  num_doc_elements  \\\n",
       "242  Granite_Foundation_Models.pdf         20          13               444   \n",
       "161  Granite_Foundation_Models.pdf         20          13               444   \n",
       "262  Granite_Foundation_Models.pdf         20          13               444   \n",
       "\n",
       "                              document_id  ext  \\\n",
       "242  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "161  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "262  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "242  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "161  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "262  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "\n",
       "                  date_acquired  pdf_convert_time  \\\n",
       "242  2024-09-20T14:37:49.189377        117.302547   \n",
       "161  2024-09-20T14:37:49.189377        117.302547   \n",
       "262  2024-09-20T14:37:49.189377        117.302547   \n",
       "\n",
       "                   source_filename  \\\n",
       "242  Granite_Foundation_Models.pdf   \n",
       "161  Granite_Foundation_Models.pdf   \n",
       "262  Granite_Foundation_Models.pdf   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "242  A. Tokenization\\nWe use the byte-level BPE tok...  $.main-text[402]   \n",
       "161  A. Foundation Model Evaluation Framework\\n4) M...  $.main-text[164]   \n",
       "262  JAPANESE FINANCE BENCHMARK EVALUATION RESULTS\\...      $.tables[11]   \n",
       "\n",
       "     page_number                                               bbox  \n",
       "242           17  [48.14419556, 380.41003418, 300.07315063, 414....  \n",
       "161            7  [48.16139603, 64.27197266, 300.24572754, 98.24...  \n",
       "262           19  [47.96198654, 486.60632324, 587.04760742, 698....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (f\"Files processed : {input_df.shape[0]:,}\")\n",
    "print (f\"Chunks created : {output_df.shape[0]:,}\")\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b9079",
   "metadata": {},
   "source": [
    "## Step-4:  DOC ID generation\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is a pre-requisite for fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e88f76",
   "metadata": {},
   "source": [
    "### 4.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7debd243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-3: Processing input='output-papers/02_chunk_out' --> output='output-papers/03_docid_out'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "STAGE  = 3\n",
    "\n",
    "input_folder = output_chunk_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_docid_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadc2f3",
   "metadata": {},
   "source": [
    "### 4.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b0eade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:38:19 INFO - Running locally\n",
      "14:38:19 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'chunk_hash', 'int_column': 'chunk_id', 'start_id': 0}\n",
      "14:38:19 INFO - data factory data_ is using local data access: input_folder - output-papers/02_chunk_out output_folder - output-papers/03_docid_out\n",
      "14:38:19 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:38:19 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "14:38:19 INFO - pipeline id pipeline_id\n",
      "14:38:19 INFO - code location None\n",
      "14:38:19 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "14:38:19 INFO - actor creation delay 0\n",
      "14:38:19 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-09-20 14:38:21,177\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=3995824)\u001b[0m 14:38:22 INFO - orchestrator started at 2024-09-20 14:38:22\n",
      "\u001b[36m(orchestrate pid=3995824)\u001b[0m 14:38:22 INFO - Number of files is 2, source profile {'max_file_size': 0.045917510986328125, 'min_file_size': 0.024618148803710938, 'total_file_size': 0.07053565979003906}\n",
      "\u001b[36m(orchestrate pid=3995824)\u001b[0m 14:38:22 INFO - Cluster resources: {'cpus': 16, 'gpus': 1, 'memory': 8.383980561047792, 'object_store': 4.191990279592574}\n",
      "\u001b[36m(orchestrate pid=3995824)\u001b[0m 14:38:22 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=3995824)\u001b[0m 14:38:22 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=3995824)\u001b[0m 14:38:22 INFO - Completed processing 2 files in 0.012 min\n",
      "\u001b[36m(orchestrate pid=3995824)\u001b[0m 14:38:22 INFO - done flushing in 0.001 sec\n",
      "14:38:32 INFO - Completed execution in 0.227 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:3 completed successfully\n",
      "CPU times: user 119 ms, sys: 146 ms, total: 265 ms\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from doc_id_transform_ray import DocIDRayTransformRuntimeConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"chunk_hash\",\n",
    "    \"doc_id_int_column\": \"chunk_id\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(DocIDRayTransformRuntimeConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5c6e4",
   "metadata": {},
   "source": [
    "### 4.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d941b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (266, 15)\n",
      "Output data dimensions (rows x columns)=  (266, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>chunk_hash</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>II. DATA SOURCES\\n17) FDIC: The data is from t...</td>\n",
       "      <td>$.main-text[43]</td>\n",
       "      <td>2</td>\n",
       "      <td>[311.07015991, 235.40817261, 563.03210449, 257...</td>\n",
       "      <td>0cc275a845bc1d6a6fc1673e4062abdbefef4cb451d45f...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>82b50c62-ad6d-4c6e-94f5-128d5c8141dc</td>\n",
       "      <td>pdf</td>\n",
       "      <td>414d95bff49753a945e0917d47b840bef75c67bd56af13...</td>\n",
       "      <td>137574</td>\n",
       "      <td>2024-09-20T14:36:45.815652</td>\n",
       "      <td>53.907655</td>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>2 Background\\nTo the best of our knowledge, ho...</td>\n",
       "      <td>$.main-text[26]</td>\n",
       "      <td>2</td>\n",
       "      <td>[107.20847321, 168.97727966, 505.65722656, 211...</td>\n",
       "      <td>c1c1dbffc522dba7a2cdc92f99cb6b21b95c74280968f6...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>82b50c62-ad6d-4c6e-94f5-128d5c8141dc</td>\n",
       "      <td>pdf</td>\n",
       "      <td>414d95bff49753a945e0917d47b840bef75c67bd56af13...</td>\n",
       "      <td>137574</td>\n",
       "      <td>2024-09-20T14:36:45.815652</td>\n",
       "      <td>53.907655</td>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>4 Why Self-Attention\\nThe third is the path le...</td>\n",
       "      <td>$.main-text[82]</td>\n",
       "      <td>6</td>\n",
       "      <td>[107.13996887, 107.34164429, 504.09875488, 182...</td>\n",
       "      <td>484beea4c33470aa1074e812a067e0da9978477a9c3746...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  num_pages  num_tables  num_doc_elements  \\\n",
       "105  Granite_Foundation_Models.pdf         20          13               444   \n",
       "16   attention-is-all-you-need.pdf         15           4               193   \n",
       "46   attention-is-all-you-need.pdf         15           4               193   \n",
       "\n",
       "                              document_id  ext  \\\n",
       "105  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "16   82b50c62-ad6d-4c6e-94f5-128d5c8141dc  pdf   \n",
       "46   82b50c62-ad6d-4c6e-94f5-128d5c8141dc  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "105  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "16   414d95bff49753a945e0917d47b840bef75c67bd56af13...  137574   \n",
       "46   414d95bff49753a945e0917d47b840bef75c67bd56af13...  137574   \n",
       "\n",
       "                  date_acquired  pdf_convert_time  \\\n",
       "105  2024-09-20T14:37:49.189377        117.302547   \n",
       "16   2024-09-20T14:36:45.815652         53.907655   \n",
       "46   2024-09-20T14:36:45.815652         53.907655   \n",
       "\n",
       "                   source_filename  \\\n",
       "105  Granite_Foundation_Models.pdf   \n",
       "16   attention-is-all-you-need.pdf   \n",
       "46   attention-is-all-you-need.pdf   \n",
       "\n",
       "                                              contents     doc_jsonpath  \\\n",
       "105  II. DATA SOURCES\\n17) FDIC: The data is from t...  $.main-text[43]   \n",
       "16   2 Background\\nTo the best of our knowledge, ho...  $.main-text[26]   \n",
       "46   4 Why Self-Attention\\nThe third is the path le...  $.main-text[82]   \n",
       "\n",
       "     page_number                                               bbox  \\\n",
       "105            2  [311.07015991, 235.40817261, 563.03210449, 257...   \n",
       "16             2  [107.20847321, 168.97727966, 505.65722656, 211...   \n",
       "46             6  [107.13996887, 107.34164429, 504.09875488, 182...   \n",
       "\n",
       "                                            chunk_hash  chunk_id  \n",
       "105  0cc275a845bc1d6a6fc1673e4062abdbefef4cb451d45f...       105  \n",
       "16   c1c1dbffc522dba7a2cdc92f99cb6b21b95c74280968f6...        16  \n",
       "46   484beea4c33470aa1074e812a067e0da9978477a9c3746...        46  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "## Step-5: Exact Dedup\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### 5.1 -  Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c7a1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-4: Processing input='output-papers/03_docid_out' --> output='output-papers/04_exact_dedupe_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  = 4\n",
    "\n",
    "input_folder = output_docid_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_exact_dedupe_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### 5.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:38:34 INFO - Running locally\n",
      "14:38:34 INFO - exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'chunk_hash', 'use_snapshot': False, 'snapshot_directory': None, 'hash_cpu': 0.5, 'num_hashes': 2}\n",
      "14:38:34 INFO - data factory data_ is using local data access: input_folder - output-papers/03_docid_out output_folder - output-papers/04_exact_dedupe_out\n",
      "14:38:34 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:38:34 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "14:38:34 INFO - pipeline id pipeline_id\n",
      "14:38:34 INFO - code location None\n",
      "14:38:34 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "14:38:34 INFO - actor creation delay 0\n",
      "14:38:34 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-09-20 14:38:36,316\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=3997362)\u001b[0m 14:38:37 INFO - orchestrator started at 2024-09-20 14:38:37\n",
      "\u001b[36m(orchestrate pid=3997362)\u001b[0m 14:38:37 INFO - Number of files is 2, source profile {'max_file_size': 0.05344200134277344, 'min_file_size': 0.028825759887695312, 'total_file_size': 0.08226776123046875}\n",
      "\u001b[36m(orchestrate pid=3997362)\u001b[0m 14:38:37 INFO - Cluster resources: {'cpus': 16, 'gpus': 1, 'memory': 8.392591095529497, 'object_store': 4.196295547299087}\n",
      "\u001b[36m(orchestrate pid=3997362)\u001b[0m 14:38:37 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=3997362)\u001b[0m 14:38:37 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=3997362)\u001b[0m 14:38:38 INFO - Completed processing 2 files in 0.013 min\n",
      "\u001b[36m(orchestrate pid=3997362)\u001b[0m 14:38:38 INFO - done flushing in 0.001 sec\n",
      "14:38:48 INFO - Completed execution in 0.227 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:4 completed successfully\n",
      "CPU times: user 127 ms, sys: 129 ms, total: 255 ms\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import ededup transform configuration\n",
    "from ededup_transform_ray import EdedupRayTransformRuntimeConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "    \"ededup_doc_id_column\": \"chunk_hash\",\n",
    "    \n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(EdedupRayTransformRuntimeConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1c3c3",
   "metadata": {},
   "source": [
    "### 5.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d824ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (266, 17)\n",
      "Output data dimensions (rows x columns)=  (266, 18)\n",
      "Input chunks before exact dedupe : 266\n",
      "Output chunks after exact dedupe : 266\n",
      "Duplicate chunks removed :   0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>chunk_hash</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>B. Granite Model Evaluation and Comparison\\nBO...</td>\n",
       "      <td>$.main-text[205]</td>\n",
       "      <td>10</td>\n",
       "      <td>[54.90453339, 486.26574707, 293.43884277, 514....</td>\n",
       "      <td>8e7cf377e1ff6cd28cd912463d39a2239aa862738f4d74...</td>\n",
       "      <td>192</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>###System:\\nVersion, JCommonsenseQA = 1.1. Ver...</td>\n",
       "      <td>$.tables[7]</td>\n",
       "      <td>18</td>\n",
       "      <td>[80.01394653, 554.28863525, 531.83129883, 604....</td>\n",
       "      <td>b24761d9d3ecd0751b33dcf8ee3f31e3405df5420efddd...</td>\n",
       "      <td>256</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>B. Pre-Processing Pipeline\\nThe output of this...</td>\n",
       "      <td>$.main-text[83]</td>\n",
       "      <td>4</td>\n",
       "      <td>[48.22718048, 471.43258667, 300.17358398, 493....</td>\n",
       "      <td>ac8d0aa73c3dbaae67893df2a6c470edc59dcd4c00791b...</td>\n",
       "      <td>122</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  num_pages  num_tables  num_doc_elements  \\\n",
       "192  Granite_Foundation_Models.pdf         20          13               444   \n",
       "256  Granite_Foundation_Models.pdf         20          13               444   \n",
       "122  Granite_Foundation_Models.pdf         20          13               444   \n",
       "\n",
       "                              document_id  ext  \\\n",
       "192  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "256  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "122  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "192  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "256  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "122  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "\n",
       "                  date_acquired  pdf_convert_time  \\\n",
       "192  2024-09-20T14:37:49.189377        117.302547   \n",
       "256  2024-09-20T14:37:49.189377        117.302547   \n",
       "122  2024-09-20T14:37:49.189377        117.302547   \n",
       "\n",
       "                   source_filename  \\\n",
       "192  Granite_Foundation_Models.pdf   \n",
       "256  Granite_Foundation_Models.pdf   \n",
       "122  Granite_Foundation_Models.pdf   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "192  B. Granite Model Evaluation and Comparison\\nBO...  $.main-text[205]   \n",
       "256  ###System:\\nVersion, JCommonsenseQA = 1.1. Ver...       $.tables[7]   \n",
       "122  B. Pre-Processing Pipeline\\nThe output of this...   $.main-text[83]   \n",
       "\n",
       "     page_number                                               bbox  \\\n",
       "192           10  [54.90453339, 486.26574707, 293.43884277, 514....   \n",
       "256           18  [80.01394653, 554.28863525, 531.83129883, 604....   \n",
       "122            4  [48.22718048, 471.43258667, 300.17358398, 493....   \n",
       "\n",
       "                                            chunk_hash  chunk_id removed  \n",
       "192  8e7cf377e1ff6cd28cd912463d39a2239aa862738f4d74...       192      []  \n",
       "256  b24761d9d3ecd0751b33dcf8ee3f31e3405df5420efddd...       256      []  \n",
       "122  ac8d0aa73c3dbaae67893df2a6c470edc59dcd4c00791b...       122      []  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (f\"Input chunks before exact dedupe : {input_df.shape[0]:,}\")\n",
    "print (f\"Output chunks after exact dedupe : {output_df.shape[0]:,}\")\n",
    "print (\"Duplicate chunks removed :  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## Step-6: Fuzzy Dedup\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### 6.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-5: Processing input='output-papers/04_exact_dedupe_out' --> output='output-papers/05_fuzzy_dedupe_out'\n"
     ]
    }
   ],
   "source": [
    "## Input to this component is the output of doc_id generator component. \n",
    "\n",
    "STAGE  = 5\n",
    "\n",
    "input_folder = output_exact_dedupe_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_fuzzy_dedupe_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### 6.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:38:49 INFO - Running locally\n",
      "14:38:49 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'chunk_id', 'cluster_column': 'chunk_hash', 'bucket_cpu': 0.3, 'mhash_cpu': 0.3, 'doc_cpu': 0.3, 'num_doc_actors': 1, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 1, 'num_permutations': 64, 'threshold': 0.7, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 1}}\n",
      "14:38:49 INFO - data factory data_ is using local data access: input_folder - output-papers/04_exact_dedupe_out output_folder - output-papers/05_fuzzy_dedupe_out\n",
      "14:38:49 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:38:49 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "14:38:49 INFO - pipeline id pipeline_id\n",
      "14:38:49 INFO - code location None\n",
      "14:38:49 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "14:38:49 INFO - actor creation delay 0\n",
      "14:38:49 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-09-20 14:38:51,531\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - orchestrator started at 2024-09-20 14:38:52\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - Number of files is 2, source profile {'max_file_size': 0.053801536560058594, 'min_file_size': 0.02918529510498047, 'total_file_size': 0.08298683166503906}\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - Cluster resources: {'cpus': 16, 'gpus': 1, 'memory': 8.390341187827289, 'object_store': 4.195170592516661}\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - Fuzzy: num buckets 8, bucket length 8\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - Table preprocessing uses 1 readers\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:52 INFO - created 1 table processor actors\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:55 INFO - Completed 1 files in 0.048 min\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:55 INFO - Completed 1 files (50.0%)  in 0.048 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:58 INFO - Completed processing 2 files in 0.099 min\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:58 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:59 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:38:59 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:00 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:00 INFO - created 1 document actors\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:00 INFO - created 1 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:00 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:00 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=3999815)\u001b[0m 14:39:00 INFO - processing buckets 0 long, 2126 short\n",
      "\u001b[36m(BucketsHash pid=3999815)\u001b[0m 14:39:00 INFO - Done submitting long buckets\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:01 INFO - Done processing buckets in 0.011 min\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:01 INFO - creating document snapshots\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=4000222)\u001b[0m 14:39:01 INFO - Waiting bucket processing completion. Submitted requests 22\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:02 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:02 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:11 INFO - Completed processing 2 files in 0.147 min\n",
      "\u001b[36m(orchestrate pid=3998966)\u001b[0m 14:39:11 INFO - done flushing in 0.001 sec\n",
      "14:39:21 INFO - Completed execution in 0.524 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:5 completed successfully\n",
      "CPU times: user 190 ms, sys: 168 ms, total: 359 ms\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"chunk_id\",\n",
    "    \"fdedup_cluster_column\": \"chunk_hash\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.3,\n",
    "    \"fdedup_doc_cpu\": 0.3,\n",
    "    \"fdedup_mhash_cpu\": 0.3,\n",
    "    \"fdedup_num_doc_actors\": 1,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 1,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.7, # between 0.0 to 1.0 ; smaller values tend to be more lenient in finding near dupes; close to 1.0 is more strict\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8cd11",
   "metadata": {},
   "source": [
    "### 6.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e899ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (266, 18)\n",
      "Output data dimensions (rows x columns)=  (266, 18)\n",
      "Duplicate chunks removed  by fuzzy-dedupe:   0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>removed</th>\n",
       "      <th>chunk_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>VI. SOCIO-TECHNICAL HARMS AND RISKS\\nThrough m...</td>\n",
       "      <td>$.main-text[228]</td>\n",
       "      <td>11</td>\n",
       "      <td>[310.98599243, 390.85067749, 563.30352783, 496...</td>\n",
       "      <td>205</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>###System:\\nTable XII summarizes the results f...</td>\n",
       "      <td>$.main-text[423]</td>\n",
       "      <td>17</td>\n",
       "      <td>[310.63598633, 43.96501541, 563.35064697, 221....</td>\n",
       "      <td>252</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>B. Downstream Documentation\\nFor downstream us...</td>\n",
       "      <td>$.main-text[236]</td>\n",
       "      <td>11</td>\n",
       "      <td>[311.23956299, 62.55028915, 563.27984619, 84.1...</td>\n",
       "      <td>210</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  num_pages  num_tables  num_doc_elements  \\\n",
       "205  Granite_Foundation_Models.pdf         20          13               444   \n",
       "252  Granite_Foundation_Models.pdf         20          13               444   \n",
       "210  Granite_Foundation_Models.pdf         20          13               444   \n",
       "\n",
       "                              document_id  ext  \\\n",
       "205  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "252  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "210  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "205  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "252  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "210  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "\n",
       "                  date_acquired  pdf_convert_time  \\\n",
       "205  2024-09-20T14:37:49.189377        117.302547   \n",
       "252  2024-09-20T14:37:49.189377        117.302547   \n",
       "210  2024-09-20T14:37:49.189377        117.302547   \n",
       "\n",
       "                   source_filename  \\\n",
       "205  Granite_Foundation_Models.pdf   \n",
       "252  Granite_Foundation_Models.pdf   \n",
       "210  Granite_Foundation_Models.pdf   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "205  VI. SOCIO-TECHNICAL HARMS AND RISKS\\nThrough m...  $.main-text[228]   \n",
       "252  ###System:\\nTable XII summarizes the results f...  $.main-text[423]   \n",
       "210  B. Downstream Documentation\\nFor downstream us...  $.main-text[236]   \n",
       "\n",
       "     page_number                                               bbox  chunk_id  \\\n",
       "205           11  [310.98599243, 390.85067749, 563.30352783, 496...       205   \n",
       "252           17  [310.63598633, 43.96501541, 563.35064697, 221....       252   \n",
       "210           11  [311.23956299, 62.55028915, 563.27984619, 84.1...       210   \n",
       "\n",
       "    removed  chunk_hash  \n",
       "205      []          -1  \n",
       "252      []          -1  \n",
       "210      []          -1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (\"Duplicate chunks removed  by fuzzy-dedupe:  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## Step-7:   Text encoding\n",
    "\n",
    "Encode text for the vector storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbbeaff",
   "metadata": {},
   "source": [
    "### 7.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-6: Processing input='output-papers/05_fuzzy_dedupe_out' --> output='output-papers/06_embeddings_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  = 6\n",
    "\n",
    "input_folder = output_fuzzy_dedupe_dir\n",
    "output_folder =  output_embeddings_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a88f8",
   "metadata": {},
   "source": [
    "### 7.2 - Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:39:22 INFO - Running locally\n",
      "14:39:22 INFO - text_encoder parameters are : {'content_column_name': 'contents', 'output_embeddings_column_name': 'embeddings', 'model_name': 'sentence-transformers/all-MiniLM-L6-v2'}\n",
      "14:39:22 INFO - data factory data_ is using local data access: input_folder - output-papers/05_fuzzy_dedupe_out output_folder - output-papers/06_embeddings_out\n",
      "14:39:22 INFO - data factory data_ max_files -1, n_sample -1\n",
      "14:39:22 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "14:39:22 INFO - pipeline id pipeline_id\n",
      "14:39:22 INFO - code location None\n",
      "14:39:22 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "14:39:22 INFO - actor creation delay 0\n",
      "14:39:22 INFO - job details {'job category': 'preprocessing', 'job name': 'text_encoder', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-09-20 14:39:24,851\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=4001033)\u001b[0m 14:39:27 INFO - orchestrator started at 2024-09-20 14:39:27\n",
      "\u001b[36m(orchestrate pid=4001033)\u001b[0m 14:39:27 INFO - Number of files is 2, source profile {'max_file_size': 0.04754638671875, 'min_file_size': 0.02595996856689453, 'total_file_size': 0.07350635528564453}\n",
      "\u001b[36m(orchestrate pid=4001033)\u001b[0m 14:39:27 INFO - Cluster resources: {'cpus': 16, 'gpus': 1, 'memory': 8.396127319894731, 'object_store': 4.198063659481704}\n",
      "\u001b[36m(orchestrate pid=4001033)\u001b[0m 14:39:27 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=4001033)\u001b[0m 14:39:27 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=4001033)\u001b[0m 14:39:39 INFO - Completed processing 2 files in 0.194 min\n",
      "\u001b[36m(orchestrate pid=4001033)\u001b[0m 14:39:39 INFO - done flushing in 0.001 sec\n",
      "14:39:49 INFO - Completed execution in 0.443 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:6 completed successfully\n",
      "CPU times: user 475 ms, sys: 221 ms, total: 697 ms\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from text_encoder_transform_ray import TextEncoderRayTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # text_encoder\n",
    "    \"text_encoder_model_name\": MY_CONFIG.EMBEDDING_MODEL,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(TextEncoderRayTransformConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734852c",
   "metadata": {},
   "source": [
    "### 7.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b1c1d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (266, 18)\n",
      "Output data dimensions (rows x columns)=  (266, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>removed</th>\n",
       "      <th>chunk_hash</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>444</td>\n",
       "      <td>09f35709-4e06-4f73-9ec7-a9d6486f3326</td>\n",
       "      <td>pdf</td>\n",
       "      <td>f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...</td>\n",
       "      <td>394452</td>\n",
       "      <td>2024-09-20T14:37:49.189377</td>\n",
       "      <td>117.302547</td>\n",
       "      <td>Granite_Foundation_Models.pdf</td>\n",
       "      <td>A. Data Clearance and Acquisition\\nFig. 3. IBM...</td>\n",
       "      <td>$.main-text[60]</td>\n",
       "      <td>3</td>\n",
       "      <td>[311.41522217, 586.98126221, 457.3286438, 595....</td>\n",
       "      <td>113</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.04205774, 0.0002038573, -0.01417764, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>82b50c62-ad6d-4c6e-94f5-128d5c8141dc</td>\n",
       "      <td>pdf</td>\n",
       "      <td>414d95bff49753a945e0917d47b840bef75c67bd56af13...</td>\n",
       "      <td>137574</td>\n",
       "      <td>2024-09-20T14:36:45.815652</td>\n",
       "      <td>53.907655</td>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>1 Introduction\\nRecurrent models typically fac...</td>\n",
       "      <td>$.main-text[19]</td>\n",
       "      <td>2</td>\n",
       "      <td>[107.33866882, 546.35638428, 504.39059448, 632...</td>\n",
       "      <td>10</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.046332713, -0.05636784, 0.029879685, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>82b50c62-ad6d-4c6e-94f5-128d5c8141dc</td>\n",
       "      <td>pdf</td>\n",
       "      <td>414d95bff49753a945e0917d47b840bef75c67bd56af13...</td>\n",
       "      <td>137574</td>\n",
       "      <td>2024-09-20T14:36:45.815652</td>\n",
       "      <td>53.907655</td>\n",
       "      <td>attention-is-all-you-need.pdf</td>\n",
       "      <td>2 Background\\nTo the best of our knowledge, ho...</td>\n",
       "      <td>$.main-text[26]</td>\n",
       "      <td>2</td>\n",
       "      <td>[107.20847321, 168.97727966, 505.65722656, 211...</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.13807338, -0.06203546, 0.013073682, 0.0048...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  num_pages  num_tables  num_doc_elements  \\\n",
       "113  Granite_Foundation_Models.pdf         20          13               444   \n",
       "10   attention-is-all-you-need.pdf         15           4               193   \n",
       "16   attention-is-all-you-need.pdf         15           4               193   \n",
       "\n",
       "                              document_id  ext  \\\n",
       "113  09f35709-4e06-4f73-9ec7-a9d6486f3326  pdf   \n",
       "10   82b50c62-ad6d-4c6e-94f5-128d5c8141dc  pdf   \n",
       "16   82b50c62-ad6d-4c6e-94f5-128d5c8141dc  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "113  f64d5f0f36008888c7862a0ec33a06255f3e71f85f02ca...  394452   \n",
       "10   414d95bff49753a945e0917d47b840bef75c67bd56af13...  137574   \n",
       "16   414d95bff49753a945e0917d47b840bef75c67bd56af13...  137574   \n",
       "\n",
       "                  date_acquired  pdf_convert_time  \\\n",
       "113  2024-09-20T14:37:49.189377        117.302547   \n",
       "10   2024-09-20T14:36:45.815652         53.907655   \n",
       "16   2024-09-20T14:36:45.815652         53.907655   \n",
       "\n",
       "                   source_filename  \\\n",
       "113  Granite_Foundation_Models.pdf   \n",
       "10   attention-is-all-you-need.pdf   \n",
       "16   attention-is-all-you-need.pdf   \n",
       "\n",
       "                                              contents     doc_jsonpath  \\\n",
       "113  A. Data Clearance and Acquisition\\nFig. 3. IBM...  $.main-text[60]   \n",
       "10   1 Introduction\\nRecurrent models typically fac...  $.main-text[19]   \n",
       "16   2 Background\\nTo the best of our knowledge, ho...  $.main-text[26]   \n",
       "\n",
       "     page_number                                               bbox  chunk_id  \\\n",
       "113            3  [311.41522217, 586.98126221, 457.3286438, 595....       113   \n",
       "10             2  [107.33866882, 546.35638428, 504.39059448, 632...        10   \n",
       "16             2  [107.20847321, 168.97727966, 505.65722656, 211...        16   \n",
       "\n",
       "    removed  chunk_hash                                         embeddings  \n",
       "113      []          -1  [-0.04205774, 0.0002038573, -0.01417764, -0.03...  \n",
       "10       []          -1  [-0.046332713, -0.05636784, 0.029879685, -0.00...  \n",
       "16       []          -1  [-0.13807338, -0.06203546, 0.013073682, 0.0048...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12630-be6b-4188-a925-77117155617b",
   "metadata": {},
   "source": [
    "## Step-8: Copy output to final output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Copied output from 'output-papers/06_embeddings_out' --> 'output-papers/output_final'\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER_FINAL, ignore_errors=True)\n",
    "shutil.copytree(src=output_folder, dst=MY_CONFIG.OUTPUT_FOLDER_FINAL)\n",
    "\n",
    "print (f\"‚úÖ Copied output from '{output_folder}' --> '{MY_CONFIG.OUTPUT_FOLDER_FINAL}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce85f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
