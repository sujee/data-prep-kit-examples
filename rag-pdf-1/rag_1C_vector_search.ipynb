{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handy Utils to do Vector Search on Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Vector Database\n",
    "\n",
    "Milvus can be embedded and easy to use.\n",
    "\n",
    "<span style=\"color:blue;\">Note: If you encounter an error about unable to load database, try this: </span>\n",
    "\n",
    "- <span style=\"color:blue;\">In **vscode** : **restart the kernel** of previous notebook. This will release the db.lock </span>\n",
    "- <span style=\"color:blue;\">In **Jupyter**: Do `File --> Close and Shutdown Notebook` of previous notebook. This will release the db.lock</span>\n",
    "- <span style=\"color:blue;\">Re-run this cell again</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance: ./rag_1_dpk.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(MY_CONFIG.DB_URI)\n",
    "\n",
    "print (\"✅ Connected to Milvus instance:\", MY_CONFIG.DB_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embeddings\n",
    "\n",
    "Two choices here. \n",
    "\n",
    "1. use sentence transformers directly\n",
    "2. use Milvus model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-rag-workshop-dev3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-rag-workshop-dev3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Option 1 - use sentence transformers directly\n",
    "\n",
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(MY_CONFIG.EMBEDDING_MODEL)\n",
    "\n",
    "def get_embeddings (str):\n",
    "    embeddings = embedding_model.encode(str, normalize_embeddings=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 2 - Milvus model\n",
    "from pymilvus import model\n",
    "\n",
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "\n",
    "# embedding_fn = model.DefaultEmbeddingFunction()\n",
    "\n",
    "## initialize the SentenceTransformerEmbeddingFunction\n",
    "embedding_fn = model.dense.SentenceTransformerEmbeddingFunction(\n",
    "    model_name = MY_CONFIG.EMBEDDING_MODEL,\n",
    "    device='cpu' # this will work on all devices (KIS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence transformer : embeddings len = 384\n",
      "sentence transformer : embeddings[:5] =  [ 0.02468893  0.10352131  0.02752644 -0.08551719 -0.01412828]\n",
      "milvus model wrapper : embeddings len = 384\n",
      "milvus model wrapper  : embeddings[:5] =  [ 0.02468893  0.10352128  0.02752643 -0.08551716 -0.01412826]\n"
     ]
    }
   ],
   "source": [
    "# Test Embeddings\n",
    "text = 'Paris 2024 Olympics'\n",
    "embeddings = get_embeddings(text)\n",
    "print ('sentence transformer : embeddings len =', len(embeddings))\n",
    "print ('sentence transformer : embeddings[:5] = ', embeddings[:5])\n",
    "\n",
    "embeddings = embedding_fn([text])\n",
    "print ('milvus model wrapper : embeddings len =', len(embeddings[0]))\n",
    "print ('milvus model wrapper  : embeddings[:5] = ', embeddings[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do A  Vector Search\n",
    "\n",
    "We will do this to verify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "## helper function to perform vector search\n",
    "def  do_vector_search (query):\n",
    "    query_vectors = [get_embeddings(query)]  # Option 1 - using sentence transformers\n",
    "    # query_vectors = embedding_fn([query])  # using Milvus model \n",
    "\n",
    "    results = milvus_client.search(\n",
    "        collection_name=MY_CONFIG.COLLECTION_NAME,  # target collection\n",
    "        data=query_vectors,  # query vectors\n",
    "        limit=5,  # number of returned entities\n",
    "        output_fields=[\"filename\", \"page_number\", \"text\"],  # specifies fields to be returned\n",
    "    )\n",
    "    return results\n",
    "## ----\n",
    "\n",
    "def  print_search_results (results):\n",
    "    # pprint (results)\n",
    "    print ('num results : ', len(results[0]))\n",
    "\n",
    "    for i, r in enumerate (results[0]):\n",
    "        #pprint(r, indent=4)\n",
    "        print (f'------ result {i+1} --------')\n",
    "        print ('search score:', r['distance'])\n",
    "        print ('filename:', r['entity']['filename'])\n",
    "        print ('page number:', r['entity']['page_number'])\n",
    "        print ('text:\\n', r['entity']['text'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Vector Search\n",
    "\n",
    "See [questions](questions.md) file for some sample questions.  You can ask your own questions, of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num results :  5\n",
      "------ result 1 --------\n",
      "search score: 0.7776628732681274\n",
      "filename: Granite_Foundation_Models.pdf\n",
      "page number: 1\n",
      "text:\n",
      " B. Overview of the Granite Pre-Training Dataset\n",
      "The Granite pre-training dataset was created as a proprietary alternative to commonly used open-source data compilations for LLM training such as \"The Pile\" [7] or \"C4\" [8]. Some\n",
      "\n",
      "------ result 2 --------\n",
      "search score: 0.7564382553100586\n",
      "filename: Granite_Foundation_Models.pdf\n",
      "page number: 15\n",
      "text:\n",
      " February 15th, 2024\n",
      "· Significant updates were made to update the paper for the latest granite model training approach and results (granite.13b.instruct.v2, granite.13b.chat.v2.1).\n",
      "\n",
      "------ result 3 --------\n",
      "search score: 0.7377078533172607\n",
      "filename: Granite_Foundation_Models.pdf\n",
      "page number: 9\n",
      "text:\n",
      " B. Granite Model Evaluation and Comparison\n",
      "TABLE II GRANITE.13B GENERAL KNOWLEDGE PERFORMANCE DURING TRAINING\n",
      "\n",
      "------ result 4 --------\n",
      "search score: 0.7191017866134644\n",
      "filename: Granite_Foundation_Models.pdf\n",
      "page number: 8\n",
      "text:\n",
      " B. Granite Model Evaluation and Comparison\n",
      "progressively training on each 100B tokens steadily improved General Knowledge.\n",
      "\n",
      "------ result 5 --------\n",
      "search score: 0.7130157947540283\n",
      "filename: Granite_Foundation_Models.pdf\n",
      "page number: 8\n",
      "text:\n",
      " B. Granite Model Evaluation and Comparison\n",
      "Fig. 6. Granite.13b General Knowledge Performance during Training.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Solar system\n",
    "# query = \"Which planet has more moons?\"\n",
    "\n",
    "# papers\n",
    "query = \"What was the training data used to train Granite model?\"\n",
    "\n",
    "## FOMC\n",
    "# query = \"Who is on the board?\"\n",
    "# query = \"Which members voted?\"\n",
    "\n",
    "results = do_vector_search (query)\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num results :  5\n",
      "------ result 1 --------\n",
      "search score: 0.6816660165786743\n",
      "filename: attention-is-all-you-need.pdf\n",
      "page number: 2\n",
      "text:\n",
      " 1 Introduction\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "------ result 2 --------\n",
      "search score: 0.6500335931777954\n",
      "filename: attention-is-all-you-need.pdf\n",
      "page number: 3\n",
      "text:\n",
      " 3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "\n",
      "------ result 3 --------\n",
      "search score: 0.584357500076294\n",
      "filename: attention-is-all-you-need.pdf\n",
      "page number: 10\n",
      "text:\n",
      " 7 Conclusion\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "\n",
      "------ result 4 --------\n",
      "search score: 0.5560237169265747\n",
      "filename: attention-is-all-you-need.pdf\n",
      "page number: 13\n",
      "text:\n",
      " Attention Visualizations Input-Input Layer5\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.\n",
      "\n",
      "------ result 5 --------\n",
      "search score: 0.553534746170044\n",
      "filename: attention-is-all-you-need.pdf\n",
      "page number: 15\n",
      "text:\n",
      " Attention Visualizations Input-Input Layer5\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Solar system\n",
    "# query = \"Is Phobos a planet?\"\n",
    "\n",
    "## papers\n",
    "query = \"What is attention mechanism?\"\n",
    "\n",
    "## FOMC\n",
    "# query = \"What is the target inflation rate?\"\n",
    "# query = \"When would the rate cut take effect?\"\n",
    "\n",
    "\n",
    "results = do_vector_search (query)\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# milvus_client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
