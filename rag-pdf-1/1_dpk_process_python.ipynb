{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Data Processing for RAG with Data Prep Kit (Python)</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15976e3",
   "metadata": {},
   "source": [
    "## Before Running the notebook\n",
    "\n",
    "Please complete [setting up python dev environment](./setup-python-dev-env.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will process PDF documents as part of RAG pipeline\n",
    "\n",
    "![](media/rag-overview-2.png)\n",
    "\n",
    "This notebook will perform steps 1, 2, 3 and 4 in RAG pipeline.\n",
    "\n",
    "Here are the processing steps:\n",
    "\n",
    "- **pdf2parquet** : Extract text (in markdown format) from PDF and store them as parquet files\n",
    "- **Exact Dedup**: Documents with exact content are filtered out\n",
    "- **Chunk documents**: Split the PDFs into 'meaningful sections' (paragraphs, sentences ..etc)\n",
    "- **Text encoder**: Convert chunks into vectors using embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b10be1",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33345487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8902eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup path to utils folder\n",
    "import sys\n",
    "sys.path.append('../utils')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb3bbc",
   "metadata": {},
   "source": [
    "## Step-2:  Data\n",
    "\n",
    "We will use white papers  about LLMs.  \n",
    "\n",
    "- [Granite Code Models](https://arxiv.org/abs/2405.04324)\n",
    "- [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "You can of course substite your own data below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe7c0c",
   "metadata": {},
   "source": [
    "### 2.1 - data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9443714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input data: ../data/papers\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "from file_utils import download_file\n",
    "\n",
    "print (\"Using input data:\", MY_CONFIG.INPUT_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8739b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# import shutil\n",
    "# from file_utils import download_file\n",
    "\n",
    "# shutil.rmtree(MY_CONFIG.INPUT_DATA_DIR, ignore_errors=True)\n",
    "# shutil.os.makedirs(MY_CONFIG.INPUT_DATA_DIR, exist_ok=True)\n",
    "# print (\"‚úÖ Cleared input directory\")\n",
    " \n",
    "# download_file (url = 'https://arxiv.org/pdf/1706.03762', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'attention.pdf' ))\n",
    "# download_file (url = 'https://arxiv.org/pdf/2405.04324', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'granite.pdf' ))\n",
    "# download_file (url = 'https://arxiv.org/pdf/2405.04324', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'granite2.pdf' )) # duplicate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### 2.2 - Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleared output directory\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(MY_CONFIG.INPUT_DATA_DIR ):\n",
    "    raise Exception (f\"‚ùå Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
    "\n",
    "output_parquet_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '01_parquet_out')\n",
    "output_exact_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '02_dedupe_out')\n",
    "# output_chunk_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '03_chunk_out')\n",
    "# output_embeddings_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '04_embeddings_out')\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print (\"‚úÖ Cleared output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "## Step-3: docling2parquet -  Convert data from PDF to Parquet\n",
    "\n",
    "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
    "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### 3.1 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b101999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-1: Processing input='../data/papers' --> output='output-papers/01_parquet_out'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"time\": \"15:57:14\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"docling2parquet parameters are : {'batch_size': -1, 'artifacts_path': None, 'contents_type': <docling2parquet_contents_types.MARKDOWN: 'text/markdown'>, 'do_table_structure': True, 'do_ocr': True, 'ocr_engine': <docling2parquet_ocr_engine.EASYOCR: 'easyocr'>, 'bitmap_area_threshold': 0.05, 'pdf_backend': <docling2parquet_pdf_backend.DLPARSE_V2: 'dlparse_v2'>, 'double_precision': 8, 'pipeline': <docling2parquet_pipeline.MULTI_STAGE: 'multi_stage'>, 'generate_picture_images': False, 'generate_page_images': False, 'images_scale': 2.0}\"}\n",
      "{\"time\": \"15:57:14\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"pipeline id pipeline_id\"}\n",
      "{\"time\": \"15:57:14\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"code location {'github': 'UNDEFINED', 'build-date': 'UNDEFINED', 'commit_hash': 'UNDEFINED', 'path': 'UNDEFINED'}\"}\n",
      "{\"time\": \"15:57:14\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"data factory data_ max_files -1, n_sample -1\"}\n",
      "{\"time\": \"15:57:14\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\"}\n",
      "{\"time\": \"15:57:14\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"data factory data_ Data Access:  DataAccessLocal\"}\n",
      "{\"time\": \"15:57:14\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"orchestrator docling2parquet started at 2025-11-16 15:57:14\"}\n",
      "{\"time\": \"15:57:14\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Number of files is 3, source profile {'max_file_size': 2.112621307373047, 'min_file_size': 1.2146415710449219, 'total_file_size': 4.541904449462891}\"}\n",
      "{\"time\": \"15:57:14\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Initializing models\"}\n",
      "2025-11-16 15:57:14,675 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 697e701a0d70f094f909a26604124fe1\n",
      "2025-11-16 15:57:14,686 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-16 15:57:14,687 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-11-16 15:57:14,699 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-16 15:57:14,701 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-11-16 15:57:14,857 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-11-16 15:57:16,654 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-11-16 15:57:17,791 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-11-16 15:57:18,167 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-16 15:57:18,180 - INFO - Going to convert document batch...\n",
      "2025-11-16 15:57:18,180 - INFO - Processing document attention.pdf\n",
      "2025-11-16 15:57:30,009 - INFO - Finished converting document attention.pdf in 11.84 sec.\n",
      "{\"time\": \"15:57:30\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Completed 1 files (33.33%) in 0.198 min\"}\n",
      "2025-11-16 15:57:30,061 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-16 15:57:30,064 - INFO - Going to convert document batch...\n",
      "2025-11-16 15:57:30,065 - INFO - Processing document granite.pdf\n",
      "2025-11-16 15:57:49,023 - INFO - Finished converting document granite.pdf in 18.96 sec.\n",
      "{\"time\": \"15:57:49\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Completed 2 files (66.67%) in 0.515 min\"}\n",
      "2025-11-16 15:57:49,085 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-16 15:57:49,088 - INFO - Going to convert document batch...\n",
      "2025-11-16 15:57:49,089 - INFO - Processing document granite2.pdf\n",
      "2025-11-16 15:58:08,038 - INFO - Finished converting document granite2.pdf in 18.95 sec.\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Completed 3 files (100.0%) in 0.832 min\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Done processing 3 files, waiting for flush() completion.\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"done flushing in 0.0 sec\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Completed execution in 0.891 min, execution result 0\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:1 completed successfully\n",
      "CPU times: user 1min 17s, sys: 2.42 s, total: 1min 19s\n",
      "Wall time: 57.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from dpk_docling2parquet import Docling2Parquet\n",
    "from dpk_docling2parquet import docling2parquet_contents_types\n",
    "\n",
    "STAGE = 1\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{MY_CONFIG.INPUT_DATA_DIR}' --> output='{output_parquet_dir}'\\n\", flush=True)\n",
    "\n",
    "result = Docling2Parquet(input_folder=MY_CONFIG.INPUT_DATA_DIR,\n",
    "                    output_folder=output_parquet_dir,\n",
    "                    data_files_to_use=['.pdf'],\n",
    "                    docling2parquet_contents_type=docling2parquet_contents_types.MARKDOWN,   # markdown\n",
    "                    ).transform()\n",
    "\n",
    "if result == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (f\"‚ùå Stage:{STAGE}  failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca790e0",
   "metadata": {},
   "source": [
    "### 3.2 -  Inspect Generated output\n",
    "\n",
    "Here we should see one entry per input file processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe59563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 3 parquet files with 3 total rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>document_hash</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>document_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>513</td>\n",
       "      <td>6ac50bf5-f455-42db-a820-7d2f895995c3</td>\n",
       "      <td>2949302674760005271</td>\n",
       "      <td>pdf</td>\n",
       "      <td>214960a61e817387f01087f0b0b323cf1ebd8035fffcab...</td>\n",
       "      <td>48981</td>\n",
       "      <td>2025-11-16T15:57:30.045742</td>\n",
       "      <td>11.843195</td>\n",
       "      <td>attention.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>granite2.pdf</td>\n",
       "      <td>## Granite Code Models: A Family of Open Found...</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>486</td>\n",
       "      <td>a71ae005-ed2b-4866-96a9-32e28f6eca0a</td>\n",
       "      <td>3127757990743433032</td>\n",
       "      <td>pdf</td>\n",
       "      <td>58342470e7d666dca0be87a15fb0552f949a5632606fe1...</td>\n",
       "      <td>121131</td>\n",
       "      <td>2025-11-16T15:58:08.089522</td>\n",
       "      <td>18.953736</td>\n",
       "      <td>granite2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>## Granite Code Models: A Family of Open Found...</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>486</td>\n",
       "      <td>c86b16c1-d83d-4366-b409-78b9884757e2</td>\n",
       "      <td>3127757990743433032</td>\n",
       "      <td>pdf</td>\n",
       "      <td>58342470e7d666dca0be87a15fb0552f949a5632606fe1...</td>\n",
       "      <td>121131</td>\n",
       "      <td>2025-11-16T15:57:49.075365</td>\n",
       "      <td>18.963363</td>\n",
       "      <td>granite.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                           contents  \\\n",
       "0  attention.pdf  Provided proper attribution is provided, Googl...   \n",
       "1   granite2.pdf  ## Granite Code Models: A Family of Open Found...   \n",
       "2    granite.pdf  ## Granite Code Models: A Family of Open Found...   \n",
       "\n",
       "   num_pages  num_tables  num_doc_elements  \\\n",
       "0         15           4               513   \n",
       "1         28          17               486   \n",
       "2         28          17               486   \n",
       "\n",
       "                            document_id        document_hash  ext  \\\n",
       "0  6ac50bf5-f455-42db-a820-7d2f895995c3  2949302674760005271  pdf   \n",
       "1  a71ae005-ed2b-4866-96a9-32e28f6eca0a  3127757990743433032  pdf   \n",
       "2  c86b16c1-d83d-4366-b409-78b9884757e2  3127757990743433032  pdf   \n",
       "\n",
       "                                                hash    size  \\\n",
       "0  214960a61e817387f01087f0b0b323cf1ebd8035fffcab...   48981   \n",
       "1  58342470e7d666dca0be87a15fb0552f949a5632606fe1...  121131   \n",
       "2  58342470e7d666dca0be87a15fb0552f949a5632606fe1...  121131   \n",
       "\n",
       "                date_acquired  document_convert_time source_filename  \n",
       "0  2025-11-16T15:57:30.045742              11.843195   attention.pdf  \n",
       "1  2025-11-16T15:58:08.089522              18.953736    granite2.pdf  \n",
       "2  2025-11-16T15:57:49.075365              18.963363     granite.pdf  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from file_utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_parquet_dir)\n",
    "\n",
    "# print (\"Output dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.head(5)\n",
    "\n",
    "## To display certain columns\n",
    "#parquet_df[['column1', 'column2', 'column3']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f900753",
   "metadata": {},
   "source": [
    "## Step-4: Eliminate Duplicate Documents\n",
    "\n",
    "We have 2 duplicate documnets here : `granite.pdf` and `granite2.pdf`.\n",
    "\n",
    "Note how the `hash` for these documents are same.\n",
    "\n",
    "We are going to perform **de-dupe**\n",
    "\n",
    "On the content of each document, a SHA256 hash is computed, followed by de-duplication of record having identical hashes.\n",
    "\n",
    "[Dedupe transform documentation](https://github.com/data-prep-kit/data-prep-kit/blob/dev/transforms/universal/ededup/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef93831",
   "metadata": {},
   "source": [
    "### 4.1 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1901b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-2: Processing input='output-papers/01_parquet_out' --> output='output-papers/02_dedupe_out'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'document_id', 'use_snapshot': False, 'snapshot_directory': None}\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"pipeline id pipeline_id\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"code location {'github': 'UNDEFINED', 'build-date': 'UNDEFINED', 'commit_hash': 'UNDEFINED', 'path': 'UNDEFINED'}\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"data factory data_ max_files -1, n_sample -1\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"data factory data_ Data Access:  DataAccessLocal\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"orchestrator ededup started at 2025-11-16 15:58:08\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Number of files is 3, source profile {'max_file_size': 0.04417991638183594, 'min_file_size': 0.020964622497558594, 'total_file_size': 0.10931110382080078}\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Starting from the beginning\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Completed 1 files (33.33%) in 0.0 min\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Completed 2 files (66.67%) in 0.0 min\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Completed 3 files (100.0%) in 0.0 min\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Done processing 3 files, waiting for flush() completion.\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"done flushing in 0.0 sec\"}\n",
      "{\"time\": \"15:58:08\", \"logger\": \"dpk\", \"logLevel\": \"INFO\", \"message\": \"Completed execution in 0.001 min, execution result 0\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:2 completed successfully\n",
      "CPU times: user 36.5 ms, sys: 4.87 ms, total: 41.4 ms\n",
      "Wall time: 37.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from dpk_ededup.transform_python import Ededup\n",
    "\n",
    "STAGE = 2\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{output_parquet_dir}' --> output='{output_exact_dedupe_dir}'\\n\", flush=True)\n",
    "\n",
    "result = Ededup(input_folder=output_parquet_dir,\n",
    "    output_folder=output_exact_dedupe_dir,\n",
    "    ededup_doc_column=\"contents\",\n",
    "    ededup_doc_id_column=\"document_id\"\n",
    "    ).transform()\n",
    "\n",
    "if result == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (f\"‚ùå Stage:{STAGE}  failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a59d2",
   "metadata": {},
   "source": [
    "### 4.2 - Inspect Generated output\n",
    "\n",
    "We would see 2 documents: `attention.pdf`  and `granite.pdf`.  The duplicate `granite.pdf` has been filtered out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0691f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 3 parquet files with 3 total rows\n",
      "Successfully read 2 parquet files with 2 total rows\n",
      "Input files before exact dedupe : 3\n",
      "Output files after exact dedupe : 2\n",
      "Duplicate files removed :   1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>document_hash</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>document_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention.pdf</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>513</td>\n",
       "      <td>6ac50bf5-f455-42db-a820-7d2f895995c3</td>\n",
       "      <td>2949302674760005271</td>\n",
       "      <td>pdf</td>\n",
       "      <td>214960a61e817387f01087f0b0b323cf1ebd8035fffcab...</td>\n",
       "      <td>48981</td>\n",
       "      <td>2025-11-16T15:57:30.045742</td>\n",
       "      <td>11.843195</td>\n",
       "      <td>attention.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>## Granite Code Models: A Family of Open Found...</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>486</td>\n",
       "      <td>c86b16c1-d83d-4366-b409-78b9884757e2</td>\n",
       "      <td>3127757990743433032</td>\n",
       "      <td>pdf</td>\n",
       "      <td>58342470e7d666dca0be87a15fb0552f949a5632606fe1...</td>\n",
       "      <td>121131</td>\n",
       "      <td>2025-11-16T15:57:49.075365</td>\n",
       "      <td>18.963363</td>\n",
       "      <td>granite.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                           contents  \\\n",
       "0  attention.pdf  Provided proper attribution is provided, Googl...   \n",
       "1    granite.pdf  ## Granite Code Models: A Family of Open Found...   \n",
       "\n",
       "   num_pages  num_tables  num_doc_elements  \\\n",
       "0         15           4               513   \n",
       "1         28          17               486   \n",
       "\n",
       "                            document_id        document_hash  ext  \\\n",
       "0  6ac50bf5-f455-42db-a820-7d2f895995c3  2949302674760005271  pdf   \n",
       "1  c86b16c1-d83d-4366-b409-78b9884757e2  3127757990743433032  pdf   \n",
       "\n",
       "                                                hash    size  \\\n",
       "0  214960a61e817387f01087f0b0b323cf1ebd8035fffcab...   48981   \n",
       "1  58342470e7d666dca0be87a15fb0552f949a5632606fe1...  121131   \n",
       "\n",
       "                date_acquired  document_convert_time source_filename  \n",
       "0  2025-11-16T15:57:30.045742              11.843195   attention.pdf  \n",
       "1  2025-11-16T15:57:49.075365              18.963363     granite.pdf  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from file_utils import read_parquet_files_as_df\n",
    "\n",
    "input_df = read_parquet_files_as_df(output_parquet_dir)\n",
    "output_df = read_parquet_files_as_df(output_exact_dedupe_dir)\n",
    "\n",
    "# print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "# print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (f\"Input files before exact dedupe : {input_df.shape[0]:,}\")\n",
    "print (f\"Output files after exact dedupe : {output_df.shape[0]:,}\")\n",
    "print (\"Duplicate files removed :  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b825b-b9cd-4608-a717-34d8abe74edd",
   "metadata": {},
   "source": [
    "## Step 5 - Save final docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c492334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear out final output folder\n",
    "\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER_FINAL, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER_FINAL, exist_ok=True)\n",
    "\n",
    "output_final_dir_parquet = os.path.join (MY_CONFIG.OUTPUT_FOLDER_FINAL, 'pq')\n",
    "shutil.os.makedirs(output_final_dir_parquet, exist_ok=True)\n",
    "\n",
    "output_final_dir_markdown = os.path.join (MY_CONFIG.OUTPUT_FOLDER_FINAL, 'markdown')\n",
    "shutil.os.makedirs(output_final_dir_markdown, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a83f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved CLEAN parquet output to 'output-papers/output_final/pq'\n"
     ]
    }
   ],
   "source": [
    "## save parquet\n",
    "\n",
    "output_df.to_parquet(os.path.join(output_final_dir_parquet, \"clean_docs.parquet\"))\n",
    "print (f\"‚úÖ Saved CLEAN parquet output to '{output_final_dir_parquet}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58e0b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved CLEAN markdown output to 'output-papers/output_final/markdown'\n"
     ]
    }
   ],
   "source": [
    "## save markdown text\n",
    "\n",
    "for index, row in output_df.iterrows():\n",
    "    output_file_name = os.path.join (output_final_dir_markdown, row['filename'] + '.md')\n",
    "    with open(output_file_name, 'w') as output_file:\n",
    "        output_file.write(row['contents'])\n",
    "\n",
    "print (f\"‚úÖ Saved CLEAN markdown output to '{output_final_dir_markdown}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpk-example-1",
   "language": "python",
   "name": "dpk-example-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
