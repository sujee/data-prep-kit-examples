{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processed Data into Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig:\n",
    "    pass\n",
    "MY_CONFIG = MyConfig()\n",
    "\n",
    "MY_CONFIG.INPUT_DATA_DIR = 'data/granite-docs/output_final/'\n",
    "# MY_CONFIG.INPUT_DATA_REMOTE = \"https://github.com/sujee/data-prep-kit-examples/blob/main/requirements.txt\"\n",
    "\n",
    "MY_CONFIG.EMBEDDING_MODEL = \"BAAI/bge-small-en-v1.5\"\n",
    "MY_CONFIG.EMBEDDING_LENGTH = 384\n",
    "\n",
    "MY_CONFIG.DB_URI = './rag_demo_dataprepkit_1.db'\n",
    "MY_CONFIG.COLLECTION_NAME = 'dataprepkit_granite_docs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-: Load Parquet Data\n",
    "\n",
    "Load all  `.parquet` files in the given dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from :  data/granite-docs/output_final/\n",
      "Number of parquet files to read :  1\n",
      "\n",
      "Read file: 'data/granite-docs/output_final/Granite_Foundation_Models.parquet'.  number of rows = 216\n",
      "\n",
      "Total number of rows = 216\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "print ('Loading data from : ', MY_CONFIG.INPUT_DATA_DIR)\n",
    "\n",
    "# Get a list of all Parquet files in the directory\n",
    "parquet_files = glob.glob(f'{MY_CONFIG.INPUT_DATA_DIR}/*.parquet')\n",
    "print (\"Number of parquet files to read : \", len(parquet_files))\n",
    "print ()\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each Parquet file and read it into a DataFrame\n",
    "for file in parquet_files:\n",
    "    df = pd.read_parquet(file)\n",
    "    print (f\"Read file: '{file}'.  number of rows = {df.shape[0]}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print (f\"\\nTotal number of rows = {data_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length:  384\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 216 entries, 0 to 215\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   filename                      216 non-null    object \n",
      " 1   num_pages                     216 non-null    int64  \n",
      " 2   num_tables                    216 non-null    int64  \n",
      " 3   num_doc_elements              216 non-null    int64  \n",
      " 4   document_id                   216 non-null    object \n",
      " 5   ext                           216 non-null    object \n",
      " 6   hash                          216 non-null    object \n",
      " 7   size                          216 non-null    int64  \n",
      " 8   date_acquired                 216 non-null    object \n",
      " 9   pdf_convert_time              216 non-null    float64\n",
      " 10  source_filename               216 non-null    object \n",
      " 11  text                          216 non-null    object \n",
      " 12  doc_jsonpath                  216 non-null    object \n",
      " 13  page_number                   216 non-null    int64  \n",
      " 14  bbox                          216 non-null    object \n",
      " 15  int_id_column                 216 non-null    int64  \n",
      " 16  hash_column                   216 non-null    int64  \n",
      " 17  ft_lang                       216 non-null    object \n",
      " 18  ft_score                      216 non-null    float64\n",
      " 19  docq_total_words              216 non-null    int64  \n",
      " 20  docq_mean_word_len            216 non-null    float64\n",
      " 21  docq_symbol_to_word_ratio     216 non-null    float64\n",
      " 22  docq_sentence_count           216 non-null    int64  \n",
      " 23  docq_lorem_ipsum_ratio        216 non-null    float64\n",
      " 24  docq_curly_bracket_ratio      216 non-null    float64\n",
      " 25  docq_contain_bad_word         216 non-null    bool   \n",
      " 26  docq_bullet_point_ratio       216 non-null    float64\n",
      " 27  docq_ellipsis_line_ratio      216 non-null    float64\n",
      " 28  docq_alphabet_word_ratio      216 non-null    float64\n",
      " 29  docq_contain_common_en_words  216 non-null    bool   \n",
      " 30  vector                        216 non-null    object \n",
      "dtypes: bool(2), float64(9), int64(9), object(11)\n",
      "memory usage: 49.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>...</th>\n",
       "      <th>docq_symbol_to_word_ratio</th>\n",
       "      <th>docq_sentence_count</th>\n",
       "      <th>docq_lorem_ipsum_ratio</th>\n",
       "      <th>docq_curly_bracket_ratio</th>\n",
       "      <th>docq_contain_bad_word</th>\n",
       "      <th>docq_bullet_point_ratio</th>\n",
       "      <th>docq_ellipsis_line_ratio</th>\n",
       "      <th>docq_alphabet_word_ratio</th>\n",
       "      <th>docq_contain_common_en_words</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Granite%20Foundation%20Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>445</td>\n",
       "      <td>efcb95d9-20bb-4997-a306-d2e27d8cccb2</td>\n",
       "      <td>pdf</td>\n",
       "      <td>be4734f0ce53ed18a69ad6cd116a8db3192c7ec7ad2a7c...</td>\n",
       "      <td>455953</td>\n",
       "      <td>2024-07-31T17:17:16.729066</td>\n",
       "      <td>37.919722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.007855909, 0.018679393, 0.042436924, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Granite%20Foundation%20Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>445</td>\n",
       "      <td>efcb95d9-20bb-4997-a306-d2e27d8cccb2</td>\n",
       "      <td>pdf</td>\n",
       "      <td>be4734f0ce53ed18a69ad6cd116a8db3192c7ec7ad2a7c...</td>\n",
       "      <td>455953</td>\n",
       "      <td>2024-07-31T17:17:16.729066</td>\n",
       "      <td>37.919722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[-0.0035767434, 0.009818679, 0.03441954, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Granite%20Foundation%20Models.pdf</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>445</td>\n",
       "      <td>efcb95d9-20bb-4997-a306-d2e27d8cccb2</td>\n",
       "      <td>pdf</td>\n",
       "      <td>be4734f0ce53ed18a69ad6cd116a8db3192c7ec7ad2a7c...</td>\n",
       "      <td>455953</td>\n",
       "      <td>2024-07-31T17:17:16.729066</td>\n",
       "      <td>37.919722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.022207903, 0.0050711804, 0.022928528, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename  num_pages  num_tables  num_doc_elements  \\\n",
       "0  Granite%20Foundation%20Models.pdf         20          13               445   \n",
       "1  Granite%20Foundation%20Models.pdf         20          13               445   \n",
       "2  Granite%20Foundation%20Models.pdf         20          13               445   \n",
       "\n",
       "                            document_id  ext  \\\n",
       "0  efcb95d9-20bb-4997-a306-d2e27d8cccb2  pdf   \n",
       "1  efcb95d9-20bb-4997-a306-d2e27d8cccb2  pdf   \n",
       "2  efcb95d9-20bb-4997-a306-d2e27d8cccb2  pdf   \n",
       "\n",
       "                                                hash    size  \\\n",
       "0  be4734f0ce53ed18a69ad6cd116a8db3192c7ec7ad2a7c...  455953   \n",
       "1  be4734f0ce53ed18a69ad6cd116a8db3192c7ec7ad2a7c...  455953   \n",
       "2  be4734f0ce53ed18a69ad6cd116a8db3192c7ec7ad2a7c...  455953   \n",
       "\n",
       "                date_acquired  pdf_convert_time  ...  \\\n",
       "0  2024-07-31T17:17:16.729066         37.919722  ...   \n",
       "1  2024-07-31T17:17:16.729066         37.919722  ...   \n",
       "2  2024-07-31T17:17:16.729066         37.919722  ...   \n",
       "\n",
       "  docq_symbol_to_word_ratio docq_sentence_count docq_lorem_ipsum_ratio  \\\n",
       "0                       0.0                   2                    0.0   \n",
       "1                       0.0                   1                    0.0   \n",
       "2                       0.0                   6                    0.0   \n",
       "\n",
       "   docq_curly_bracket_ratio docq_contain_bad_word  docq_bullet_point_ratio  \\\n",
       "0                       0.0                 False                      0.0   \n",
       "1                       0.0                 False                      0.0   \n",
       "2                       0.0                 False                      0.0   \n",
       "\n",
       "   docq_ellipsis_line_ratio docq_alphabet_word_ratio  \\\n",
       "0                       0.0                      1.0   \n",
       "1                       0.0                      1.0   \n",
       "2                       0.0                      1.0   \n",
       "\n",
       "   docq_contain_common_en_words  \\\n",
       "0                          True   \n",
       "1                         False   \n",
       "2                          True   \n",
       "\n",
       "                                              vector  \n",
       "0  [-0.007855909, 0.018679393, 0.042436924, -0.01...  \n",
       "1  [-0.0035767434, 0.009818679, 0.03441954, -0.00...  \n",
       "2  [-0.022207903, 0.0050711804, 0.022928528, -0.0...  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Shape the data\n",
    "\n",
    "MY_CONFIG.EMBEDDING_LENGTH =  len(data_df.iloc[0]['embeddings'])\n",
    "print ('embedding length: ', MY_CONFIG.EMBEDDING_LENGTH)\n",
    "\n",
    "# rename 'embeddings' columns as 'vector' to match default schema\n",
    "# if 'vector' not in data_df.columns and 'embeddings' in data_df.columns:\n",
    "#     data_df = data_df.rename( columns= {'embeddings' : 'vector'})\n",
    "# if 'text' not in data_df.columns and 'contents' in data_df.columns:\n",
    "#     data_df = data_df.rename( columns= {'contents' : 'text'})\n",
    "\n",
    "data_df = data_df.rename( columns= {'embeddings' : 'vector', 'contents' : 'text'})\n",
    "\n",
    "print (data_df.info())\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Vector Database\n",
    "\n",
    "Milvus can be embedded and easy to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance: ./rag_demo_dataprepkit_1.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(MY_CONFIG.DB_URI)\n",
    "\n",
    "print (\"✅ Connected to Milvus instance:\", MY_CONFIG.DB_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create A Collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created collection :  dataprepkit_granite_docs\n"
     ]
    }
   ],
   "source": [
    "# if we already have a collection, clear it first\n",
    "if client.has_collection(collection_name=MY_CONFIG.COLLECTION_NAME):\n",
    "    client.drop_collection(collection_name=MY_CONFIG.COLLECTION_NAME)\n",
    "    print ('✅ Cleared collection :', MY_CONFIG.COLLECTION_NAME)\n",
    "\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=MY_CONFIG.COLLECTION_NAME,\n",
    "    dimension=MY_CONFIG.EMBEDDING_LENGTH,\n",
    "    metric_type=\"IP\",  # Inner product distance\n",
    "    consistency_level=\"Strong\",  # Strong consistency level\n",
    "    auto_id=True\n",
    ")\n",
    "print (\"✅ Created collection : \", MY_CONFIG.COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df.to_dict('records')[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted # rows 216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'row_count': 216}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = client.insert(collection_name=MY_CONFIG.COLLECTION_NAME, data=data_df.to_dict('records'))\n",
    "\n",
    "print('inserted # rows', res['insert_count'])\n",
    "\n",
    "client.get_collection_stats(MY_CONFIG.COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do A Simple Vector Search\n",
    "\n",
    "We will do this to verify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import model\n",
    "import random\n",
    "\n",
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "# import os\n",
    "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# embedding_fn = model.DefaultEmbeddingFunction()\n",
    "\n",
    "## initialize the SentenceTransformerEmbeddingFunction\n",
    "embedding_fn = model.dense.SentenceTransformerEmbeddingFunction(\n",
    "    model_name = MY_CONFIG.EMBEDDING_MODEL,\n",
    "    device='cpu' # this will work on all devices (KIS)\n",
    ")\n",
    "\n",
    "## helper function to perform vector search\n",
    "def  do_vector_search (query):\n",
    "    # query_vectors = embedding_fn.encode_queries([query])\n",
    "    query_vectors = embedding_fn([query])\n",
    "\n",
    "    results = client.search(\n",
    "        collection_name=MY_CONFIG.COLLECTION_NAME,  # target collection\n",
    "        data=query_vectors,  # query vectors\n",
    "        limit=5,  # number of returned entities\n",
    "        output_fields=[\"filename\", \"page_number\", \"text\"],  # specifies fields to be returned\n",
    "    )\n",
    "    return results\n",
    "## ----\n",
    "\n",
    "def  print_search_results (results):\n",
    "    # pprint (results)\n",
    "    print ('num results : ', len(results[0]))\n",
    "\n",
    "    for i, r in enumerate (results[0]):\n",
    "        #pprint(r, indent=4)\n",
    "        print (i+1)\n",
    "        print ('search score:', r['distance'])\n",
    "        print ('filename:', r['entity']['filename'])\n",
    "        print ('page number:', r['entity']['page_number'])\n",
    "        print ('text:', r['entity']['text'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num results :  5\n",
      "1\n",
      "search score: 0.8775781393051147\n",
      "filename: Granite%20Foundation%20Models.pdf\n",
      "page number: 9\n",
      "text: B. Granite Model Evaluation and Comparison\n",
      "TABLE II GRANITE.13B GENERAL KNOWLEDGE PERFORMANCE DURING TRAINING\n",
      "\n",
      "2\n",
      "search score: 0.8280448913574219\n",
      "filename: Granite%20Foundation%20Models.pdf\n",
      "page number: 9\n",
      "text: B. Granite Model Evaluation and Comparison\n",
      "For the SocialStigmaQA benchmark, we tested a variety of the Granite, llama-2, and flan-ul2 models. We examine whether the inclusion of specific personal attributes in the prompt leads\n",
      "\n",
      "3\n",
      "search score: 0.8216304779052734\n",
      "filename: Granite%20Foundation%20Models.pdf\n",
      "page number: 8\n",
      "text: B. Granite Model Evaluation and Comparison\n",
      "Fig. 6. Granite.13b General Knowledge Performance during Training.\n",
      "\n",
      "4\n",
      "search score: 0.817500114440918\n",
      "filename: Granite%20Foundation%20Models.pdf\n",
      "page number: 2\n",
      "text: C. Organization of Report\n",
      "The remainder of this report is organized as follows. In Section II, we describe the data sources used in granite.13b's pre-training. In Section III, we describe the data processing steps we undertake with a focus on the governance steps we follow. In Section IV, we provide further details about the pre-training and fine-tuning algorithms, the computation involved, and the energy consumption we estimate. Section V presents the testing and evaluation framework along with quantitative comparisons to other models. In Section VI, we discuss our approach to understanding and mitigating sociotechnical harms from the Granite models. Section VII provides a brief discussion of the usage policies and the socio-technical documentation of Granite models. Finally in Section VIII, we conclude with areas of future work and discussion.\n",
      "\n",
      "5\n",
      "search score: 0.801689088344574\n",
      "filename: Granite%20Foundation%20Models.pdf\n",
      "page number: 4\n",
      "text: IV. TRAINING\n",
      "In this section, we detail the training process for the decoderonly Granite models covering the algorithmic details of pretraining and fine-tuning, the computing involved, and an estimate of the carbon footprint.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Overview of the Granite Pre-Training Dataset\"\n",
    "\n",
    "results = do_vector_search (query)\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num results :  5\n",
      "1\n",
      "search score: 0.8625919818878174\n",
      "filename: Granite%20Foundation%20Models.pdf\n",
      "page number: 3\n",
      "text: III. DATA GOVERNANCE\n",
      "Fig. 2. Summary governance statistics on IBM's curated pre-training dataset at the time of granite.13b.v2's training.\n",
      "\n",
      "2\n",
      "search score: 0.8519487380981445\n",
      "filename: Granite%20Foundation%20Models.pdf\n",
      "page number: 3\n",
      "text: III. DATA GOVERNANCE\n",
      "A. Data clearance and acquisition;\n",
      "B. Pre-processing; and\n",
      "C. Tokenization.\n",
      "\n",
      "3\n",
      "search score: 0.8367217183113098\n",
      "filename: Granite%20Foundation%20Models.pdf\n",
      "page number: 3\n",
      "text: III. DATA GOVERNANCE\n",
      "Data governance is organized into the following processes, corresponding to data lifecycle phases prior to model training:\n",
      "\n",
      "4\n",
      "search score: 0.8130795955657959\n",
      "filename: Granite%20Foundation%20Models.pdf\n",
      "page number: 2\n",
      "text: III. DATA GOVERNANCE\n",
      "As IBM is making Granite models available to customers to adapt to their own applications, we have invested heavily in a data governance process that evaluates datasets for governance, risk and compliance (GRC) criteria, including IBM's standard data clearance process, document quality checks, and other\n",
      "\n",
      "5\n",
      "search score: 0.7999685406684875\n",
      "filename: Granite%20Foundation%20Models.pdf\n",
      "page number: 3\n",
      "text: III. DATA GOVERNANCE\n",
      "criteria. IBM has developed governance procedures for LLM pre-training datasets which are consistent with IBM AI Ethics principles and are guided by the IBM Corporate Legal Team. Best practices around LLM development is continually evolving with the ever-increasing understanding of AI models, their usage, and changing regulatory requirements, among other factors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Data governance\"\n",
    "\n",
    "results = do_vector_search (query)\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-prep-kit-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
